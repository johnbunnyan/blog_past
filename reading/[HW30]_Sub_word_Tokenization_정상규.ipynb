{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "53c64692130147d3a40c59a8ec76d7af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d2f4a68d823148e2818dc87211b6dc96",
              "IPY_MODEL_940b0fd6f709456babf2ca443b9385a1",
              "IPY_MODEL_2975ad7c26a84cd9bb9f8aecafa4048c"
            ],
            "layout": "IPY_MODEL_35c58310a6c54ba798d80bf78e0fbb76"
          }
        },
        "d2f4a68d823148e2818dc87211b6dc96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a15ce6811781488b94ae0d6ca9c32665",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ab1c3277c22c4b09a80e15055caaa8c9",
            "value": "Downloading:  19%"
          }
        },
        "940b0fd6f709456babf2ca443b9385a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_098e394801604737b922f52a160b706f",
            "max": 213450,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e8d2bb42363e42a696cf35c102e832f5",
            "value": 40960
          }
        },
        "2975ad7c26a84cd9bb9f8aecafa4048c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35657777f67741389992f9d5717393f7",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ee757f442a834600aebdd153ff8c6d7c",
            "value": " 41.0k/213k [00:00&lt;00:01, 105kB/s]"
          }
        },
        "35c58310a6c54ba798d80bf78e0fbb76": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a15ce6811781488b94ae0d6ca9c32665": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab1c3277c22c4b09a80e15055caaa8c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "098e394801604737b922f52a160b706f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8d2bb42363e42a696cf35c102e832f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "35657777f67741389992f9d5717393f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee757f442a834600aebdd153ff8c6d7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johnbunnyan/johnbunnyan.github.io/blob/main/reading/%5BHW30%5D_Sub_word_Tokenization_%EC%A0%95%EC%83%81%EA%B7%9C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOnz8OxdbN3y"
      },
      "source": [
        "# Tokenization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZb_LOZr7XF_"
      },
      "source": [
        "### Data Preparation\n",
        "\n",
        "\n",
        "*   ë°ì´í„°(train, dev, test)ê°€ ì €ì¥ëœ ë“œë¼ì´ë¸Œë¥¼ ë§ˆìš´íŠ¸í•˜ì—¬ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆëŠ” ê²½ë¡œë¥¼ ì¤€ë¹„í•©ë‹ˆë‹¤.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkCrAHWVmUck",
        "outputId": "2f2b20b9-42ce-43b2-eb73-9ff826dce156",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os, sys\n",
        "from google.colab import drive\n",
        "\n",
        "# ì¶”ê°€ì ì¸ ìë£Œê°€ í•„ìš”í•œ ê²½ìš°\n",
        "# ì˜¤ë¥¸ìª½ í´ë” ë‚´ë¶€ì— í•´ë‹¹ ìë£Œë¥¼ ì¶”ê°€í•  í•„ìš” ìˆë‹¤\n",
        "# ê·¸ëŸ¬ë ¤ë©´ êµ¬ê¸€ ë“œë¼ì´ë¸Œì— í•´ë‹¹ ë“œë¼ì´ë¸Œë¥¼ ë„£ì–´ì•¼í•˜ê³ \n",
        "# êµ¬ê¸€ì½”ë©ì˜ driveë¡œ ì—°ê²° \n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ì‚¬ìš©í•  íŒŒì¼ê²½ë¡œë¥¼ osë¡œ ì„¤ì •\n",
        "# chdir : 'ch'ange 'dir'ectory í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ë¥¼ ë³€ê²½í•œë‹¤.\n",
        "# í˜„ì¬ íŒŒì¼ ê²½ë¡œì— ë§ê²Œ ì„¤ì •í•´ì£¼ì„¸ìš”.\n",
        "os.chdir('/content/drive/MyDrive/Colab Notebooks/colab_[HW 30] Sub-word Tokenization')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9yYsxMfyZsi",
        "outputId": "c37a03c3-4eae-4d6b-e282-6feb11f645cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# í˜„ì¬ ê²½ë¡œ í™•ì¸\n",
        "!pwd"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/colab_[HW 30] Sub-word Tokenization\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceOZZ8UOU4or",
        "outputId": "12d3f250-b975-423c-a010-2dd7d98cd64b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
        "!pip install transformers"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m94.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.2)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m182.4/182.4 KB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSNON1TAbj2i"
      },
      "source": [
        "### Introduction\n",
        "\n",
        "\n",
        "* ë³¸ ê³¼ì œì˜ ëª©ì ì€ Subword tokenizationì˜ í•„ìš”ì„±ì„ ì§ì ‘ ëŠê»´ë³´ëŠ” ê²ƒì…ë‹ˆë‹¤.\n",
        "* Subword tokenization ê¸°ë°˜ language modelì„ êµ¬í˜„í•˜ë©´ì„œ ì´ì „ ê³¼ì œì˜ Word-level language modelê³¼ ë¹„êµí•´ë³´ëŠ” ì‹œê°„ì„ ê°–ê² ìŠµë‹ˆë‹¤. ì¶”ê°€ì ìœ¼ë¡œ RNNì„ LSTMìœ¼ë¡œ ë³€ê²½í–ˆì„ ë•Œì˜ ì„±ëŠ¥ ì°¨ì´ì— ëŒ€í•´ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.\n",
        "*   Subword-level language modelì„ êµ¬í˜„í•˜ê³ , ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ ê°€ê³µí•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµí•œ í›„ í•™ìŠµëœ ì–¸ì–´ ëª¨ë¸ì„ ì´ìš©í•´ ë¬¸ì¥ì„ ìƒì„±í•©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xzSMF9RJyzg"
      },
      "source": [
        "\n",
        "```\n",
        "ğŸ’¡ SubwordëŠ” ë¬´ì—‡ì¸ê°€ìš”?\n",
        "\n",
        "SubwordëŠ” í•˜ë‚˜ì˜ ë‹¨ì–´ë¥¼ ì—¬ëŸ¬ê°œì˜ ë‹¨ìœ„ë¡œ ë¶„ë¦¬í–ˆì„ ë•Œ í•˜ë‚˜ì˜ ë‹¨ìœ„ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. \"subword\"ë¥¼ subword ë‹¨ìœ„ë¡œ ë‚˜íƒ€ë‚¸ í•˜ë‚˜ì˜ ì˜ˆì‹œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
        "\n",
        "\"sub\" + \"word\"\n",
        "\n",
        "subë¼ëŠ” ì ‘ë‘ì‚¬ì™€ wordë¼ê³  í•˜ëŠ” ì–´ê·¼ìœ¼ë¡œ ë‚˜ëˆ„ì–´ \"subword\"ë¼ê³  í•˜ëŠ” wordë¥¼ 2ê°œì˜ subwordë¡œ ë‚˜íƒ€ëƒˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "ì´ì™¸ì—ë„ ë‹¤ì–‘í•œ í˜•íƒœì˜ subwordë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. (e.g., \"su\" + \"bword\", \"s\" + \"ubword\", \"subwor\" + \"d\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrgY-yRfVJON"
      },
      "source": [
        "```\n",
        "ğŸ’¡ tokenizationì€ ë¬´ì—‡ì¸ê°€ìš”?\n",
        "\n",
        "tokenizationì€ ì£¼ì–´ì§„ ì…ë ¥ ë°ì´í„°ë¥¼ ìì—°ì–´ì²˜ë¦¬ ëª¨ë¸ì´ ì¸ì‹í•  ìˆ˜ ìˆëŠ” ë‹¨ìœ„ë¡œ ë³€í™˜í•´ì£¼ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. \n",
        "\n",
        "ğŸ’¡ word tokenizationì€ìš”?\n",
        "\n",
        "word tokenizationì˜ ê²½ìš° \"ë‹¨ì–´\"ê°€ ìì—°ì–´ì²˜ë¦¬ ëª¨ë¸ì´ ì¸ì‹í•˜ëŠ” ë‹¨ìœ„ê°€ ë©ë‹ˆë‹¤.\n",
        "\"I have a meal\"ì´ë¼ê³  í•˜ëŠ” ë¬¸ì¥ì„ ê°€ì§€ê³  word tokenizationì„ í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. \n",
        "\n",
        "- ['I', 'have', 'a', 'meal']\n",
        "\n",
        "ì˜ì–´ì˜ ê²½ìš° ëŒ€ë¶€ë¶„ spaceë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‹¨ì–´ê°€ ì •ì˜ë˜ê¸° ë•Œë¬¸ì— .split()ì„ ì´ìš©í•´ ì‰½ê²Œ word tokenizationì„ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "ì˜ì–´ì—ì„œ word tokenizationì€ space tokenizationì´ë¼ê³ ë„ í•  ìˆ˜ ìˆê³ , \n",
        "subword tokenization ì´ì „ì— ìˆ˜í–‰ë˜ëŠ” pre-tokenization ë°©ë²•ìœ¼ë¡œë„ ë§ì´ ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
        "\n",
        "\"ë‚˜ëŠ” ë°¥ì„ ë¨¹ëŠ”ë‹¤\"ë¼ëŠ” ë¬¸ì¥ì„ word tokenizationí•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
        "- ['ë‚˜', 'ëŠ”', 'ë°¥', 'ì„', 'ë¨¹ëŠ”ë‹¤']\n",
        "\n",
        "í•œêµ­ì–´ì—ì„œ \"ë‹¨ì–´\"ëŠ” ê³µë°±(space)ì„ ê¸°ì¤€ìœ¼ë¡œ ì •ì˜ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì´ëŠ” í•œêµ­ì–´ê°€ ê°–ê³  ìˆëŠ” \"êµì°©ì–´\"ë¡œì„œì˜ íŠ¹ì§• ë•Œë¬¸ì…ë‹ˆë‹¤. \n",
        "ì²´ì–¸ ë’¤ì— ì¡°ì‚¬ê°€ ë¶™ëŠ” ê²ƒì´ ëŒ€í‘œì ì¸ íŠ¹ì§•ì´ë©° ì˜ë¯¸ ë‹¨ìœ„ê°€ êµ¬ë¶„ë˜ê³  ìë¦½ì„±ì´ ìˆê¸° ë•Œë¬¸ì— ì¡°ì‚¬ëŠ” \"ë‹¨ì–´\"ì…ë‹ˆë‹¤.\n",
        "\n",
        "í•œêµ­ì–´ì—ì„œëŠ” pre-tokenization ë°©ë²•ìœ¼ë¡œ space tokenizationì„ ì‚¬ìš©í•˜ì§€ ì•Šê³  í˜•íƒœì†Œ ë¶„ì„ê¸°ë¥¼ í™œìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
        "```\n",
        "\n",
        "(ì°¸ê³ 1: [êµ­ë¦½ êµ­ì–´ì›: \"ì¡°ì‚¬ëŠ” ë‹¨ì–´ì´ë‹¤\"](https://www.korean.go.kr/front/onlineQna/onlineQnaView.do?mn_id=216&qna_seq=26915#:~:text='%EC%A1%B0%EC%82%AC'%EB%8A%94%20%EC%99%84%EC%A0%84%ED%95%9C%20%EC%9E%90%EB%A6%BD%EC%84%B1%EC%9D%80,%ED%95%98%EC%97%AC%20%EB%8B%A8%EC%96%B4%EB%A1%9C%20%EC%B2%98%EB%A6%AC%ED%95%A9%EB%8B%88%EB%8B%A4.) )\n",
        "\n",
        "(ì°¸ê³ 2: [Huggingface: Pre-tokenization](https://huggingface.co/docs/tokenizers/python/latest/pipeline.html#pre-tokenization))\n",
        "\n",
        "(ì°¸ê³ 3: [Konlpy: í˜•íƒœì†Œ ë¶„ì„ê¸°](https://konlpy-ko.readthedocs.io/ko/v0.4.3/morph/))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVQgjQJdAbWh"
      },
      "source": [
        "```\n",
        "ğŸ’¡ ê·¸ëŸ¼ Subword tokenizationì€ ë¬´ì—‡ì¸ê°€ìš”?\n",
        "\n",
        "Subword tokenizaitonì€ ë§ ê·¸ëŒ€ë¡œ subword ë‹¨ìœ„ë¡œ tokenizationì„ í•œë‹¤ëŠ” ëœ»ì…ë‹ˆë‹¤.\n",
        "ë°©ê¸ˆ ì „ word tokenizationì„ ìˆ˜í–‰í–ˆë˜ ë¬¸ì¥ì„ ì´ìš©í•´ subword tokenizationì„ ìˆ˜í–‰í•œ ì˜ˆì‹œë¥¼ ë³´ê² ìŠµë‹ˆë‹¤.\n",
        "\n",
        "Subword tokenizationì„ ì ìš©í–ˆì„ ë•ŒëŠ” ë‹¤ìŒê³¼ ê°™ì´ tokenizationì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "Example 1\n",
        "\n",
        "\"I have a meal\" -> ['I', 'hav', 'e', 'a', 'me', 'al']\n",
        "\"ë‚˜ëŠ” ë°¥ì„ ë¨¹ëŠ”ë‹¤\" -> ['ë‚˜', 'ëŠ”', 'ë°¥', 'ì„', 'ë¨¹ëŠ”', 'ë‹¤']\n",
        "\n",
        "word ë‹¨ìœ„ê°€ ì•„ë‹ˆë¼ ê·¸ë³´ë‹¤ ë” ì˜ê²Œ ìª¼ê°  subword ë‹¨ìœ„ë¡œ ë¬¸ì¥ì„ tokenizationí•©ë‹ˆë‹¤.\n",
        "\n",
        "ìœ„ì—ì„œ ë§ì”€ë“œë¦° ê²ƒê³¼ ê°™ì´ ì—¬ëŸ¬ê°€ì§€ ê²½ìš°ì˜ ìˆ˜ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n",
        "\n",
        "Example 2\n",
        "\n",
        "\"I have a meal\" -> ['I', 'ha', 've', 'a', 'mea', 'l']\n",
        "\"ë‚˜ëŠ” ë°¥ì„ ë¨¹ëŠ”ë‹¤\" -> ['ë‚˜', 'ëŠ”', 'ë°¥', 'ì„', 'ë¨¹', 'ëŠ”ë‹¤']\n",
        "\n",
        "ê·¸ë ‡ì§€ë§Œ ê¸°ë³¸ì ìœ¼ë¡œ ê³µë°±ì„ ë„˜ì–´ì„  subwordë¥¼ êµ¬ì„±í•˜ì§„ ì•ŠìŠµë‹ˆë‹¤.\n",
        "ì˜ˆë¥¼ ë“¤ì–´ ë‹¤ìŒê³¼ ê°™ì´ tokenizaitonì„ ìˆ˜í–‰í•˜ì§„ ì•ŠìŠµë‹ˆë‹¤.\n",
        "\n",
        "Example 3\n",
        "\n",
        "\"I have a meal\" -> ['Iha', 've', 'am', 'ea', 'l']\n",
        "\"ë‚˜ëŠ” ë°¥ì„ ë¨¹ëŠ”ë‹¤\" -> ['ë‚˜ëŠ”ë°¥', 'ì„ë¨¹', 'ëŠ”ë‹¤']\n",
        "```\n",
        "\n",
        "(ì°¸ê³ 4: [Huggingface: subword-tokenization](https://huggingface.co/transformers/tokenizer_summary.html#subword-tokenization))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPRNaFhMEK67"
      },
      "source": [
        "```\n",
        "ğŸ’¡ Subword tokenizationì€ ì™œ í•„ìš”í•œê°€ìš”?\n",
        "\n",
        "word tokenization ì½”ë“œë¥¼ ë¶ˆëŸ¬ì™€ ê·¸ í•„ìš”ì„±ì„ ìƒê°í•´ ë´…ì‹œë‹¤.\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DESsQzhwGVST"
      },
      "source": [
        "import os\n",
        "from io import open\n",
        "import torch"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWd09aUBGWa9"
      },
      "source": [
        "class Dictionary(object):\n",
        "    def __init__(self):\n",
        "        # idx2wordì— ìˆëŠ” ë°°ì—´ì˜ ì¸ë±ìŠ¤\n",
        "        self.word2idx = {'<unk>': 0}\n",
        "        # ì‹¤ì§ˆì ì¸ ë‹¨ì–´ê°€ ì €ì¥ë˜ëŠ” ë°°ì—´\n",
        "        self.idx2word = ['<unk>']\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2idx:\n",
        "            self.idx2word.append(word)\n",
        "            self.word2idx[word] = len(self.idx2word) - 1\n",
        "        return self.word2idx[word]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx2word)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQIyGQlfd9Os"
      },
      "source": [
        "# ë§ë­‰ì¹˜ ë˜ëŠ” ì½”í¼ìŠ¤ëŠ” ìì—°ì–¸ì–´ ì—°êµ¬ë¥¼ ìœ„í•´ íŠ¹ì •í•œ ëª©ì ì„ ê°€ì§€ê³  ì–¸ì–´ì˜ í‘œë³¸ì„ ì¶”ì¶œí•œ ì§‘í•©\n",
        "# ê¸°ë³¸ì ìœ¼ë¡œ ë”•ì…”ë„ˆë¦¬ì™€ ë°ì´í„°ì…‹ ê°€ì§€ê³  ìˆë‹¤\n",
        "# ë°ì´í„°ì…‹ì€ í† í¬ë‚˜ì´ì¦ˆë¥¼ í•´ì„œ ë³´ê´€\n",
        "class Corpus(object):\n",
        "    def __init__(self, path):\n",
        "        self.dictionary = Dictionary()\n",
        "        \"\"\"Tokenizes a text file.\"\"\"\n",
        "        assert os.path.exists(path) #pathìˆëŠ”ì§€ ì²´í¬\n",
        "        # Add words to the dictionary\n",
        "        with open(os.path.join(path, 'train.txt'), 'r', encoding=\"utf8\") as f:\n",
        "            # íŒŒì¼ì—ì„œ í•œ lineì”© ì½ëŠ”ë‹¤\n",
        "            for line in f: \n",
        "                # [] <- ë¼ì¸ì„ ì˜ë¼ì„œ ë‹¨ì–´ë¡œ ë§Œë“¤ê³  ë§¨ ë’¤ì— ë¬¸ì¥ëí† í°ì„ ì¶”ê°€í•œë‹¤\n",
        "                words = line.split() + ['<eos>']\n",
        "                for word in words:\n",
        "                    self.dictionary.add_word(word)\n",
        "\n",
        "        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n",
        "        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n",
        "        self.test = self.tokenize(os.path.join(path, 'test.txt'))\n",
        "\n",
        "    def tokenize(self, path):\n",
        "        # Tokenize file content\n",
        "        with open(path, 'r', encoding=\"utf8\") as f:\n",
        "            idss = []\n",
        "            # ë‘ë²ˆì§¸ íŒŒë¼ë¯¸í„°ë¡œ ì¤€ íŒŒì¼ ê¸°ì¤€\n",
        "            for line in f:\n",
        "                words = line.split() + ['<eos>']\n",
        "                ids = []\n",
        "                for word in words:\n",
        "                    try:\n",
        "                        # dictionaryì—ì„œ ë‹¨ì–´ ì°¾ì•„ì„œ idsë°°ì—´ì— ë„£ìŒ\n",
        "                        # ì´ë•Œ dictionaryì— ìˆëŠ” ë‹¨ì–´ëŠ” trainì˜ ë‹¨ì–´ë“¤\n",
        "                        ids.append(self.dictionary.word2idx[word])\n",
        "                    except:\n",
        "                        print(word)\n",
        "                        ids.append(0) \n",
        "                # ê·¸ëƒ¥ ë°°ì—´ì„ í…ì„œë¡œ\n",
        "                idss.append(torch.tensor(ids).type(torch.int64))\n",
        "            ids = torch.cat(idss)\n",
        "\n",
        "        return ids"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a55D0z53eLzB"
      },
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class RNNModel(nn.Module):\n",
        "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
        "\n",
        "    def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=0.5):\n",
        "        #ë¶€ëª¨ í´ë˜ìŠ¤(nn.Module)ë¥¼ ìì‹ í´ë˜ìŠ¤(RNNModel)ì— ë¶ˆëŸ¬ì˜¤ê² ë‹¤ \n",
        "        super(RNNModel, self).__init__()\n",
        "        self.ntoken = ntoken\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.encoder = nn.Embedding(ntoken, ninp)\n",
        "        \n",
        "        # self.rnn ì„ ì„¤ì •\n",
        "        # rnn_typeì— ë”°ë¼ ë¶„ê¸° (LSTM,GRU/)\n",
        "        if rnn_type in ['LSTM', 'GRU']:\n",
        "                    # getattr = nn.rnn_type\n",
        "            self.rnn = getattr(nn, rnn_type)(ninp, nhid, nlayers, dropout=dropout)\n",
        "        else:\n",
        "            try:\n",
        "                nonlinearity = {'RNN_TANH': 'tanh', 'RNN_RELU': 'relu'}[rnn_type]\n",
        "            except KeyError:\n",
        "                raise ValueError( \"\"\"An invalid option for `--model` was supplied,\n",
        "                                 options are ['LSTM', 'GRU', 'RNN_TANH' or 'RNN_RELU']\"\"\")\n",
        "            self.rnn = nn.RNN(ninp, nhid, nlayers, nonlinearity=nonlinearity, dropout=dropout)\n",
        "        \n",
        "        self.decoder = nn.Linear(nhid, ntoken)\n",
        "        #\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "        self.rnn_type = rnn_type\n",
        "        self.nhid = nhid\n",
        "        self.nlayers = nlayers\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        # uniform_ : Fills the input Tensor with values drawn from the uniform distribution \\mathcal{U}(a, b)U(a,b).\n",
        "          # tensor (Tensor) â€“ an n-dimensional torch.Tensor\n",
        "          # a (float) â€“ the lower bound of the uniform distribution\n",
        "          # b (float) â€“ the upper bound of the uniform distribution\n",
        "        nn.init.uniform_(self.encoder.weight, -initrange, initrange)\n",
        "        # zeros_ : Fills the input Tensor with the scalar value 0.\n",
        "        nn.init.zeros_(self.decoder.weight)\n",
        "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        emb = self.drop(self.encoder(input))\n",
        "        output, hidden = self.rnn(emb, hidden)\n",
        "        output = self.drop(output)\n",
        "        decoded = self.decoder(output)\n",
        "        decoded = decoded.view(-1, self.ntoken)\n",
        "        return F.log_softmax(decoded, dim=1), hidden\n",
        "\n",
        "    def init_hidden(self, bsz):\n",
        "        # nextëŠ” ì´í„°ë ˆì´í„°ì—ì„œ ê°’ì„ ì°¨ë¡€ëŒ€ë¡œ êº¼ëƒ…ë‹ˆë‹¤\n",
        "        weight = next(self.parameters())\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            return (weight.new_zeros(self.nlayers, bsz, self.nhid),\n",
        "                    weight.new_zeros(self.nlayers, bsz, self.nhid))\n",
        "        else:\n",
        "            return weight.new_zeros(self.nlayers, bsz, self.nhid)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDQTvkdJdZOk"
      },
      "source": [
        "import time\n",
        "import math\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "# EasyDict allows to access dict values as attributes (works recursively). A Javascript-like properties dot notation for python dicts.\n",
        "import easydict\n",
        "\n",
        "args = easydict.EasyDict({\n",
        "    \"data\"    : './data/wikitext-2',    # location of the data corpus\n",
        "    \"model\"   : 'RNN_TANH',             # type of recurrent net (RNN_TANH, RNN_RELU, LSTM, GRU)\n",
        "    \"emsize\"  : 200,                    # size of word embeddings\n",
        "    \"nhid\"    : 512,                    # number of hidden units per layer\n",
        "    \"nlayers\" : 2,                      # number of layers\n",
        "    \"lr\"      : 20,                     # initial learning rate\n",
        "    \"clip\"    : 0.25,                   # gradient clipping\n",
        "    \"epochs\"  : 6,                      # upper epoch limit\n",
        "    \"batch_size\": 20,                   # batch size\n",
        "    \"bptt\"    : 35,                     # sequence length\n",
        "    \"dropout\" : 0.2,                    # dropout applied to layers (0 = no dropout)\n",
        "    \"seed\"    : 1111,                   # random seed\n",
        "    \"cuda\"    : True,                   # use CUDA\n",
        "    \"log_interval\": 200,                # report interval\n",
        "    \"save\"    : 'model.pt',             # path to save the final model\n",
        "    \"dry_run\" : True,                   # verify the code and the model\n",
        "\n",
        "})\n",
        "\n",
        "# ë””ë°”ì´ìŠ¤ ì„¤ì •\n",
        "device = torch.device(\"cuda\" if args.cuda else \"cpu\")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnsZkQqUHb6Z"
      },
      "source": [
        "```\n",
        "train.txtì˜ ë¬¸ì¥ë“¤ì„ word tokenization í•´ë³´ê³  ë‹¨ì–´ë“¤ì˜ ê°œìˆ˜ë¥¼ ì„¸ì–´ë³´ê² ìŠµë‹ˆë‹¤\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aSB9Hk4HjyO",
        "outputId": "3c70c511-db5f-4575-a390-2668dc08a93c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# íŒŒë¼ë¯¸í„° í†µí•´ Corpusë¥¼ ì‹¤í–‰\n",
        "corpus = Corpus('./data/wikitext-2')\n",
        "ntokens = len(corpus.dictionary)\n",
        "print(ntokens)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "33278\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBkvRpEcKEvd"
      },
      "source": [
        "```\n",
        "ì´ì „ ê³¼ì œì— ì‚¬ìš©ëœ embedding dimensionì˜ í¬ê¸°ëŠ” 200ì´ë¯€ë¡œ word embeddingì— ì‚¬ìš©ëœ parameterì˜ ìˆ˜ëŠ” 33278 x 200 (6,655,600ê°œ)ì…ë‹ˆë‹¤.\n",
        "\n",
        "ê·¸ë ‡ë‹¤ë©´, RNN ëª¨ë¸ì— ì‚¬ìš©ë˜ëŠ” weightì˜ parameter ê°œìˆ˜ëŠ” ëª‡ê°œì¸ì§€ ê°„ë‹¨í•œ í•¨ìˆ˜ë¥¼ ì´ìš©í•´ í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGjG2j-fKbJu"
      },
      "source": [
        "model = RNNModel(args.model, ntokens, args.emsize, args.nhid, args.nlayers, args.dropout)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTiKccVXLrvx"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvRPOxLCLByf",
        "outputId": "19cc026c-1528-447f-932d-6be83e997ba3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(f\"Word embedding parameter ê°œìˆ˜: {count_parameters(model.encoder)}\")\n",
        "print(f\"RNN parameter ê°œìˆ˜: {count_parameters(model.rnn)}\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word embedding parameter ê°œìˆ˜: 6655600\n",
            "RNN parameter ê°œìˆ˜: 890880\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlUFvgTNM1Od"
      },
      "source": [
        "```\n",
        "ğŸ’¡ RNN parameter, Word embedding parameter ê°œìˆ˜ë¥¼ ë¹„êµí•´ë³´ë©´ word embedding parameterì˜ ê°œìˆ˜ê°€ RNN ëª¨ë¸ì˜ parameterë³´ë‹¤ ì••ë„ì ìœ¼ë¡œ ë§ìŠµë‹ˆë‹¤.\n",
        "\n",
        "word embeddingì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš° trainingì— ì‚¬ìš©ë˜ëŠ” text fileì˜ í¬ê¸°ê°€ ì»¤ì§ˆìˆ˜ë¡ word embedding parameterëŠ” \n",
        "ë” ì»¤ì§€ê²Œ ë˜ê³  ì „ì²´ parameter ëŒ€ë¹„ word embeddingì´ ì°¨ì§€í•˜ëŠ” ë¹„ì¤‘ì€ ë§¤ìš° ë†’ì•„ì§‘ë‹ˆë‹¤.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7WfyYBrPpca"
      },
      "source": [
        "```\n",
        "âœ¨ ì´ëŸ° parameter ë¹„ì¤‘ì˜ ë¹„ëŒ€ì¹­ì„±ì„ í•´ê²°í•˜ê¸° ìœ„í•´ ì²˜ìŒì—ëŠ” character-level tokenization ë°©ë²•ì´ ì£¼ëª©ì„ ë°›ì•˜ìŠµë‹ˆë‹¤. \n",
        "ë§ ê·¸ëŒ€ë¡œ í•˜ë‚˜ì˜ ê¸€ìë¥¼ ê¸°ì¤€ìœ¼ë¡œ tokenizationì„ í•˜ëŠ”ê±´ë°ìš”.\n",
        "ì´ì „ ì˜ˆì‹œë¥¼ character ê¸°ë°˜ tokenizationì„ í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
        "\n",
        "\"I have a meal\" -> ['I', 'h', 'a', 'v', 'e', 'a', 'm', 'e', 'a', 'l']\n",
        "\"ë‚˜ëŠ” ë°¥ì„ ë¨¹ëŠ”ë‹¤\" -> ['ë‚˜', 'ëŠ”', 'ë°¥', 'ì„', 'ë¨¹', 'ëŠ”', 'ë‹¤']\n",
        "\n",
        "ê·¸ëŸ¬ë‚˜, character ê¸°ë°˜ tokenization ì—­ì‹œ ì§€ë‚˜ì¹˜ê²Œ ê¸´ sequence ê¸¸ì´, ì„±ëŠ¥ ì €í•˜ ë“±ì˜ ë¬¸ì œë¥¼ ê²ªìœ¼ë©° \n",
        "subword tokenizationì´ ê°ê´‘ì„ ë°›ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "â“ word tokenization ê¸°ë²•ê³¼ êµ¬ë¶„ë˜ëŠ” subword tokenizationì˜ ì¥ì ì„ í•˜ë‚˜ë§Œ ë” ìƒê°í•´ë³¼ê¹Œìš”?\n",
        "\n",
        "subword tokenizationì˜ ì¥ì ì€ Out-of-vocabulary (OOV) ë¬¸ì œì—ì„œ ìƒëŒ€ì ìœ¼ë¡œ ììœ ë¡­ë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤.\n",
        "\n",
        "ì¼ë°˜ì ìœ¼ë¡œ subwordë“¤ì€ ìµœì†Œ ì² ì ë‹¨ìœ„ì—ì„œ í•˜ë‚˜ì”© ë” ê¸´ subwordë¥¼ ì¶”ê°€í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë§Œë“¤ì–´ì§‘ë‹ˆë‹¤.\n",
        "\n",
        "ì˜ˆë¥¼ ë“¤ì–´, ì˜ì–´ì˜ ê²½ìš° a~zì˜ ì•ŒíŒŒë²³ë¶€í„° ì‹œì‘í•´ì„œ ë‘ê¸€ì, ì„¸ê¸€ì, ë„¤ê¸€ì subword ë“±ìœ¼ë¡œ í™•ì¥í•´ë‚˜ê°€ë©° \n",
        "subwordë¥¼ ì¶”ê°€í•´ ë‹¨ì–´ë¥¼ êµ¬ì„±í•˜ê³  ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ subword tokenizationì„ ìˆ˜í–‰í•˜ê¸° ë•Œë¬¸ì— ë‹¤ë¥¸ ì–¸ì–´ë¥¼ tokenizationí•˜ì§€ ì•ŠëŠ”ë‹¤ë©´ \n",
        "OOV ë¬¸ì œì—ì„œ ììœ ë¡­ë‹¤ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "ëŒ€í‘œì ì¸ subword tokenizationì— ì‚¬ìš©ë˜ëŠ” algorithm ì¤‘ í•˜ë‚˜ì¸ byte pair encodingì€ ì„ íƒ ê³¼ì œ 3ì—ì„œ ì‚´í´ë³¼ ìˆ˜ ìˆìœ¼ë‹ˆ ì°¸ê³ í•´ì£¼ì„¸ìš” !\n",
        "\n",
        "âœ¨ ê·¸ëŸ¼ ì´ì œë¶€í„° BERT ëª¨ë¸ì—ì„œ ì‚¬ìš©í•œ subword tokenization algorithmì„ ì´ìš©í•´ language modeling taskë¥¼ ìˆ˜í–‰í•´ë³´ê² ìŠµë‹ˆë‹¤. \n",
        "subword tokenizerëŠ” transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì´ìš©í•´ ì‰½ê²Œ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "```\n",
        "\n",
        "(ì°¸ê³ 5: [Huggingface: subword tokenization](https://huggingface.co/transformers/tokenizer_summary.html#subword-tokenization))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZKyn3PKUuBg",
        "outputId": "a5f09a4a-1de5-4cd6-df12-6bddfc5e6cb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "53c64692130147d3a40c59a8ec76d7af",
            "d2f4a68d823148e2818dc87211b6dc96",
            "940b0fd6f709456babf2ca443b9385a1",
            "2975ad7c26a84cd9bb9f8aecafa4048c",
            "35c58310a6c54ba798d80bf78e0fbb76",
            "a15ce6811781488b94ae0d6ca9c32665",
            "ab1c3277c22c4b09a80e15055caaa8c9",
            "098e394801604737b922f52a160b706f",
            "e8d2bb42363e42a696cf35c102e832f5",
            "35657777f67741389992f9d5717393f7",
            "ee757f442a834600aebdd153ff8c6d7c"
          ]
        }
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/213k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "53c64692130147d3a40c59a8ec76d7af"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwubp25xVUt2"
      },
      "source": [
        "# subword tokenization ì˜ˆì‹œ\n",
        "print(tokenizer.tokenize('Natural language expert training course'))\n",
        "print(tokenizer.tokenize('Goorm X KAIST'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLfVolP3UKos"
      },
      "source": [
        "class Corpus(object):\n",
        "    def __init__(self, path):\n",
        "        self.dictionary = Dictionary()\n",
        "        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n",
        "        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n",
        "        self.test = self.tokenize(os.path.join(path, 'test.txt'))\n",
        "\n",
        "    def tokenize(self, path):\n",
        "        assert os.path.exists(path)\n",
        "        # Add words to the dictionary\n",
        "        with open(path, 'r', encoding=\"utf8\") as f:\n",
        "            for line in f:\n",
        "                words = tokenizer.tokenize(line.strip()) + ['<eos>']\n",
        "                for word in words:\n",
        "                    self.dictionary.add_word(word)\n",
        "\n",
        "        # Tokenize file content\n",
        "        with open(path, 'r', encoding=\"utf8\") as f:\n",
        "            idss = []\n",
        "            for line in f:\n",
        "                words = tokenizer.tokenize(line.strip()) + ['<eos>']\n",
        "                ids = []\n",
        "                for word in words:\n",
        "                    ids.append(self.dictionary.word2idx[word])\n",
        "                idss.append(torch.tensor(ids).type(torch.int64))\n",
        "            ids = torch.cat(idss)\n",
        "\n",
        "        return ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvtarB2DYiLb"
      },
      "source": [
        "```\n",
        "ê·¸ëŸ¬ë©´ ì´ì œ ë‹¤ì‹œ ëª¨ë¸ì„ ì„ ì–¸í•˜ê³  parameterì˜ ê°œìˆ˜ë¥¼ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLKqis0hY5Or"
      },
      "source": [
        "subword_corpus = Corpus('./data/wikitext-2')\n",
        "ntokens = len(subword_corpus.dictionary)\n",
        "subwordmodel = RNNModel(args.model, ntokens, args.emsize, args.nhid, args.nlayers, args.dropout)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RB4DQ-QZCBd"
      },
      "source": [
        "print(f\"Word embedding parameter ê°œìˆ˜: {count_parameters(subwordmodel.encoder)}\")\n",
        "print(f\"RNN parameter ê°œìˆ˜: {count_parameters(subwordmodel.rnn)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_5y4M6k1HR6"
      },
      "source": [
        "```\n",
        "ì´ì „ì— ë¹„í•´ embedding parameter ê°œìˆ˜ëŠ” í™•ì—°íˆ ì¤„ì–´ë“¤ì—ˆìŠµë‹ˆë‹¤.\n",
        "6,655,600ê°œ -> 4,619,000ê°œ\n",
        "\n",
        "ê·¸ëŸ¬ë©´ ì´ì œ subword ê¸°ë°˜ì˜ ì–¸ì–´ ëª¨ë¸ ì„±ëŠ¥ì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlSdSCKvd23M"
      },
      "source": [
        "###############################################################################\n",
        "# Load data\n",
        "###############################################################################\n",
        "\n",
        "# Starting from sequential data, batchify arranges the dataset into columns.\n",
        "# For instance, with the alphabet as the sequence and batch size 4, we'd get\n",
        "# â”Œ a g m s â”\n",
        "# â”‚ b h n t â”‚\n",
        "# â”‚ c i o u â”‚\n",
        "# â”‚ d j p v â”‚\n",
        "# â”‚ e k q w â”‚\n",
        "# â”” f l r x â”˜.\n",
        "# These columns are treated as independent by the model, which means that the\n",
        "# dependence of e. g. 'g' on 'f' can not be learned, but allows more efficient\n",
        "# batch processing.\n",
        "\n",
        "def batchify(data, bsz):\n",
        "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
        "    nbatch = data.size(0) // bsz\n",
        "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
        "    data = data.narrow(0, 0, nbatch * bsz)\n",
        "    # Evenly divide the data across the bsz batches.\n",
        "    data = data.view(bsz, -1).t().contiguous()\n",
        "    return data.to(device)\n",
        "\n",
        "eval_batch_size = 10\n",
        "train_data = batchify(subword_corpus.train, args.batch_size)\n",
        "val_data = batchify(subword_corpus.valid, eval_batch_size)\n",
        "test_data = batchify(subword_corpus.test, eval_batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7iRv2rHe959"
      },
      "source": [
        "###############################################################################\n",
        "# Build the model\n",
        "###############################################################################\n",
        "\n",
        "model = RNNModel(args.model, ntokens, args.emsize, args.nhid, args.nlayers, args.dropout).to(device)\n",
        "criterion = nn.NLLLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HiLaw_Bte_nJ"
      },
      "source": [
        "###############################################################################\n",
        "# Training code1 - define functions\n",
        "###############################################################################\n",
        "\n",
        "def repackage_hidden(h):\n",
        "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
        "\n",
        "    if isinstance(h, torch.Tensor):\n",
        "        return h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)\n",
        "\n",
        "\n",
        "# get_batch subdivides the source data into chunks of length args.bptt.\n",
        "# If source is equal to the example output of the batchify function, with\n",
        "# a bptt-limit of 2, we'd get the following two Variables for i = 0:\n",
        "# â”Œ a g m s â” â”Œ b h n t â”\n",
        "# â”” b h n t â”˜ â”” c i o u â”˜\n",
        "# Note that despite the name of the function, the subdivison of data is not\n",
        "# done along the batch dimension (i.e. dimension 1), since that was handled\n",
        "# by the batchify function. The chunks are along dimension 0, corresponding\n",
        "# to the seq_len dimension in the LSTM.\n",
        "\n",
        "def get_batch(source, i):\n",
        "    seq_len = min(args.bptt, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].view(-1)\n",
        "    return data, target\n",
        "\n",
        "\n",
        "def evaluate(data_source):\n",
        "    # Turn on evaluation mode which disables dropout.\n",
        "    model.eval()\n",
        "    total_loss = 0.\n",
        "    ntokens = len(subword_corpus.dictionary)\n",
        "    hidden = model.init_hidden(eval_batch_size)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, data_source.size(0) - 1, args.bptt):\n",
        "            data, targets = get_batch(data_source, i)\n",
        "            output, hidden = model(data, hidden)\n",
        "            hidden = repackage_hidden(hidden)\n",
        "            total_loss += len(data) * criterion(output, targets).item()\n",
        "    return total_loss / (len(data_source) - 1)\n",
        "\n",
        "\n",
        "def train():\n",
        "    # Turn on training mode which enables dropout.\n",
        "    model.train()\n",
        "    total_loss = 0.\n",
        "    start_time = time.time()\n",
        "    ntokens = len(subword_corpus.dictionary)\n",
        "    hidden = model.init_hidden(args.batch_size)\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, args.bptt)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
        "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
        "        model.zero_grad()\n",
        "\n",
        "        hidden = repackage_hidden(hidden)\n",
        "        output, hidden = model(data, hidden)\n",
        "\n",
        "        loss = criterion(output, targets)\n",
        "        loss.backward()\n",
        "\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
        "        for p in model.parameters():\n",
        "            p.data.add_(p.grad, alpha=-lr)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch % args.log_interval == 0 and batch > 0:\n",
        "            cur_loss = total_loss / args.log_interval\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n",
        "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "                epoch, batch, len(train_data) // args.bptt, lr,\n",
        "                elapsed * 1000 / args.log_interval, cur_loss, math.exp(cur_loss)))\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "        if args.dry_run:\n",
        "            break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6m-cdbm7PvS"
      },
      "source": [
        "###############################################################################\n",
        "# Training code2 - run \n",
        "###############################################################################\n",
        "\n",
        "# Loop over epochs.\n",
        "lr = args.lr\n",
        "best_val_loss = None\n",
        "\n",
        "# At any point you can hit Ctrl + C to break out of training early.\n",
        "try:\n",
        "    for epoch in range(1, args.epochs+1):\n",
        "        epoch_start_time = time.time()\n",
        "        train()\n",
        "        val_loss = evaluate(val_data)\n",
        "        print('-' * 89)\n",
        "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                           val_loss, math.exp(val_loss)))\n",
        "        print('-' * 89)\n",
        "        # Save the model if the validation loss is the best we've seen so far.\n",
        "        if not best_val_loss or val_loss < best_val_loss:\n",
        "            with open(args.save, 'wb') as f:\n",
        "                torch.save(model, f)\n",
        "            best_val_loss = val_loss\n",
        "        else:\n",
        "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
        "            lr /= 4.0\n",
        "except KeyboardInterrupt:\n",
        "    print('-' * 89)\n",
        "    print('Exiting from training early')\n",
        "\n",
        "# Load the best saved model.\n",
        "with open(args.save, 'rb') as f:\n",
        "    model = torch.load(f)\n",
        "    # after load the rnn params are not a continuous chunk of memory\n",
        "    # this makes them a continuous chunk, and will speed up forward pass\n",
        "    # Currently, only rnn model supports flatten_parameters function.\n",
        "    if args.model in ['RNN_TANH', 'RNN_RELU', 'LSTM', 'GRU']:\n",
        "        model.rnn.flatten_parameters()\n",
        "\n",
        "# Run on test data.\n",
        "test_loss = evaluate(test_data)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
        "    test_loss, math.exp(test_loss)))\n",
        "print('=' * 89)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpv4N8wl4w8p"
      },
      "source": [
        "### 4. í•™ìŠµí•œ ì–¸ì–´ ëª¨ë¸ë¡œ ë¬¸ì¥ ìƒì„±\n",
        "\n",
        "\n",
        "*   í•™ìŠµì´ ì™„ë£Œëœ ëª¨ë¸ì„ ë¶ˆëŸ¬ì™€ random í•œ ë‹¨ì–´ë¥¼ input ìœ¼ë¡œ ë„£ì–´ì¤€ í›„ ì •í•´ì§„ ê°œìˆ˜ì˜ ë‹¨ì–´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "*   ìƒì„±í•œ ë¬¸ì¥ì„ decode í•˜ì—¬ (ì¦‰, idx2word ë¥¼ ì´ìš©í•´ id ë¥¼ word ë¡œ ë³€í™˜í•˜ì—¬) generate.txt íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQjeaKjO4JAh"
      },
      "source": [
        "###############################################################################\n",
        "# Language Modeling on Wikitext-2\n",
        "#\n",
        "# This file generates new sentences sampled from the language model\n",
        "#\n",
        "###############################################################################\n",
        "\n",
        "import torch\n",
        "\n",
        "# Model parameters.\n",
        "test_args = easydict.EasyDict({\n",
        "    \"data\"      : './data/wikitext-2',  # location of data corpus\n",
        "    \"checkpoint\": './model.pt',         # model checkpoint to use\n",
        "    \"outf\"      : 'generate.txt',       # output file for generated text\n",
        "    \"words\"     : 1000,                 # number of words to generate\n",
        "    \"seed\"      : 1111,                 # random seed\n",
        "    \"cuda\"      : True,                 # use CUDA\n",
        "    \"temperature\": 1.0,                 # temperature - higher will increase diversity\n",
        "    \"log_interval\": 100                 # reporting interval\n",
        "})\n",
        "\n",
        "# Set the random seed manually for reproducibility.\n",
        "torch.manual_seed(test_args.seed)\n",
        "if torch.cuda.is_available():\n",
        "    if not test_args.cuda:\n",
        "        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
        "\n",
        "device = torch.device(\"cuda\" if test_args.cuda else \"cpu\")\n",
        "\n",
        "if test_args.temperature < 1e-3:\n",
        "    parser.error(\"--temperature has to be greater or equal 1e-3\")\n",
        "\n",
        "with open(test_args.checkpoint, 'rb') as f:\n",
        "    model = torch.load(f).to(device)\n",
        "model.eval()\n",
        "\n",
        "# corpus = Corpus(test_args.data)\n",
        "# ntokens = len(subword_corpus.dictionary)\n",
        "\n",
        "hidden = model.init_hidden(1)\n",
        "input = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\n",
        "\n",
        "with open(test_args.outf, 'w') as outf:\n",
        "    with torch.no_grad():  # no tracking history\n",
        "        for i in range(test_args.words):\n",
        "            output, hidden = model(input, hidden)\n",
        "            word_weights = output.squeeze().div(test_args.temperature).exp().cpu()\n",
        "            word_idx = torch.multinomial(word_weights, 1)[0]\n",
        "            input.fill_(word_idx)\n",
        "\n",
        "            word = subword_corpus.dictionary.idx2word[word_idx]\n",
        "\n",
        "            outf.write(word + ('\\n' if i % 20 == 19 else ' '))\n",
        "\n",
        "            if i % test_args.log_interval == 0:\n",
        "                print('| Generated {}/{} words'.format(i, test_args.words))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqLfZB-1wHHK"
      },
      "source": [
        "## Reference\n",
        "[Pytorch Language Model](https://github.com/pytorch/examples/tree/master/word_language_model)\n"
      ]
    }
  ]
}