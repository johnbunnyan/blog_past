{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johnbunnyan/johnbunnyan.github.io/blob/main/reading/%5BHW26_Answer%5DWord2Vec_%EC%A0%95%EC%83%81%EA%B7%9C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBNc6OwE3__L"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3FAK0fz1kOr"
      },
      "source": [
        "##**3. Word2Vec**\n",
        "1. 주어진 단어들을 word2vec 모델에 들어갈 수 있는 형태로 만듭니다.\n",
        "2. CBOW, Skip-gram 모델을 각각 구현합니다.\n",
        "3. 모델을 실제로 학습해보고 결과를 확인합니다.\n",
        "4. 산점도를 그려 단어들의 대략적인 위치를 확인해봅니다"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9FrxTPWIsct"
      },
      "source": [
        "### **필요 패키지 import**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utBdiiW499DI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6b56728-b5cc-434a-a111-9acb4086b71d"
      },
      "source": [
        "\n",
        "!sudo apt-get install -y fonts-nanum\n",
        "!sudo fc-cache -fv\n",
        "!rm ~/.cache/matplotlib -rf"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  fonts-nanum\n",
            "0 upgraded, 1 newly installed, 0 to remove and 20 not upgraded.\n",
            "Need to get 9,604 kB of archives.\n",
            "After this operation, 29.5 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 fonts-nanum all 20170925-1 [9,604 kB]\n",
            "Fetched 9,604 kB in 3s (3,575 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package fonts-nanum.\n",
            "(Reading database ... 124016 files and directories currently installed.)\n",
            "Preparing to unpack .../fonts-nanum_20170925-1_all.deb ...\n",
            "Unpacking fonts-nanum (20170925-1) ...\n",
            "Setting up fonts-nanum (20170925-1) ...\n",
            "Processing triggers for fontconfig (2.12.6-0ubuntu2) ...\n",
            "/usr/share/fonts: caching, new cache contents: 0 fonts, 1 dirs\n",
            "/usr/share/fonts/truetype: caching, new cache contents: 0 fonts, 3 dirs\n",
            "/usr/share/fonts/truetype/humor-sans: caching, new cache contents: 1 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/liberation: caching, new cache contents: 16 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/nanum: caching, new cache contents: 10 fonts, 0 dirs\n",
            "/usr/local/share/fonts: caching, new cache contents: 0 fonts, 0 dirs\n",
            "/root/.local/share/fonts: skipping, no such directory\n",
            "/root/.fonts: skipping, no such directory\n",
            "/var/cache/fontconfig: cleaning cache directory\n",
            "/root/.cache/fontconfig: not cleaning non-existent cache directory\n",
            "/root/.fontconfig: not cleaning non-existent cache directory\n",
            "fc-cache: succeeded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjroCdtwI9Rz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca6acb1f-5656-4d38-8047-fc415f6b92c2"
      },
      "source": [
        "!pip install konlpy"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 32.6 MB/s \n",
            "\u001b[?25hCollecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.4.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (465 kB)\n",
            "\u001b[K     |████████████████████████████████| 465 kB 73.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from konlpy) (4.9.2)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.8/dist-packages (from konlpy) (1.21.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from JPype1>=0.7.0->konlpy) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->JPype1>=0.7.0->konlpy) (3.0.9)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.4.1 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSP7aXfJIr3i"
      },
      "source": [
        "from tqdm import tqdm\n",
        "# KoNLPy 한국어 처리 패키지\n",
        "from konlpy.tag import Mecab,Twitter,Okt,Kkma \n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import defaultdict\n",
        "\n",
        "import torch\n",
        "import copy\n",
        "import numpy as np"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qugro74yJASr"
      },
      "source": [
        "### **데이터 전처리**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q36dfSRRJDtX"
      },
      "source": [
        "\n",
        "\n",
        "데이터를 확인하고 Word2Vec 형식에 맞게 전처리합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLZ2f-lRJSus"
      },
      "source": [
        "train_data = [\n",
        "  \"정말 맛있습니다. 추천합니다.\",\n",
        "  \"기대했던 것보단 별로였네요.\",\n",
        "  \"다 좋은데 가격이 너무 비싸서 다시 가고 싶다는 생각이 안 드네요.\",\n",
        "  \"완전 최고입니다! 재방문 의사 있습니다.\",\n",
        "  \"음식도 서비스도 다 만족스러웠습니다.\",\n",
        "  \"위생 상태가 좀 별로였습니다. 좀 더 개선되기를 바랍니다.\",\n",
        "  \"맛도 좋았고 직원분들 서비스도 너무 친절했습니다.\",\n",
        "  \"기념일에 방문했는데 음식도 분위기도 서비스도 다 좋았습니다.\",\n",
        "  \"전반적으로 음식이 너무 짰습니다. 저는 별로였네요.\",\n",
        "  \"위생에 조금 더 신경 썼으면 좋겠습니다. 조금 불쾌했습니다.\"       \n",
        "]\n",
        "\n",
        "test_words = [\"음식\", \"맛\", \"서비스\", \"위생\", \"가격\"]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vReElaFSLBYL"
      },
      "source": [
        "Tokenization과 vocab을 만드는 과정은 이전 실습과 유사합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTjlRzmWMDK_"
      },
      "source": [
        "# Okt(Open Korean Text)는 트위터에서 만든 오픈소스 한국어 처리기인 twitter-korean-text를 이어받아 만들고 있는 프로젝트\n",
        "tokenizer = Okt()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DTUsX672icp"
      },
      "source": [
        "def make_tokenized(data):\n",
        "  tokenized = []\n",
        "  for sent in tqdm(data):\n",
        "    tokens = tokenizer.morphs(sent, stem=True)\n",
        "    tokenized.append(tokens)\n",
        "\n",
        "  return tokenized"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-z0z6HD2rrX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53438726-4ae7-41d0-cf9b-44b83a81a4ec"
      },
      "source": [
        "train_tokenized = make_tokenized(train_data)\n",
        "# train_tokenized\n",
        "#  \"정말 맛있습니다. 추천합니다.\",\n",
        "#   \"기대했던 것보단 별로였네요.\",\n",
        "# ->\n",
        "# [] <- ['정말', '맛있다', '.', '추천', '하다', '.'],\n",
        "#  ['기대하다', '것', '보단', '별로', '이다', '.'],"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 127.43it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['정말', '맛있다', '.', '추천', '하다', '.'],\n",
              " ['기대하다', '것', '보단', '별로', '이다', '.'],\n",
              " ['다',\n",
              "  '좋다',\n",
              "  '가격',\n",
              "  '이',\n",
              "  '너무',\n",
              "  '비싸다',\n",
              "  '다시',\n",
              "  '가다',\n",
              "  '싶다',\n",
              "  '생각',\n",
              "  '이',\n",
              "  '안',\n",
              "  '드네',\n",
              "  '요',\n",
              "  '.'],\n",
              " ['완전', '최고', '이다', '!', '재', '방문', '의사', '있다', '.'],\n",
              " ['음식', '도', '서비스', '도', '다', '만족스럽다', '.'],\n",
              " ['위생',\n",
              "  '상태',\n",
              "  '가',\n",
              "  '좀',\n",
              "  '별로',\n",
              "  '이다',\n",
              "  '.',\n",
              "  '좀',\n",
              "  '더',\n",
              "  '개선',\n",
              "  '되다',\n",
              "  '기르다',\n",
              "  '바라다',\n",
              "  '.'],\n",
              " ['맛', '도', '좋다', '직원', '분들', '서비스', '도', '너무', '친절하다', '.'],\n",
              " ['기념일', '에', '방문', '하다', '음식', '도', '분위기', '도', '서비스', '도', '다', '좋다', '.'],\n",
              " ['전반', '적', '으로', '음식', '이', '너무', '짜다', '.', '저', '는', '별로', '이다', '.'],\n",
              " ['위생', '에', '조금', '더', '신경', '써다', '좋다', '.', '조금', '불쾌하다', '.']]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51exEpI0Mc3l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aae9bb48-cef0-4245-9ef0-8d39c7196111"
      },
      "source": [
        "# 모든 단어들에 대한 카운트 딕셔너리 만들기\n",
        "word_count = defaultdict(int)\n",
        "\n",
        "for tokens in tqdm(train_tokenized):\n",
        "  for token in tokens:\n",
        "    word_count[token] += 1"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 13005.59it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyvHAMAnMh1D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41fee0a0-da42-4846-c0c0-fe19a671e574"
      },
      "source": [
        "# key=lambda x: x[1] : 어떤 것을 기준으로 정렬할 것인가?, 카운트 횟수를 기준으로 정렬하겠다\n",
        "# reverse=True : 내림차순으로 정렬\n",
        "word_count = sorted(word_count.items(), key=lambda x: x[1], reverse=True)\n",
        "print(list(word_count))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('.', 14), ('도', 7), ('이다', 4), ('좋다', 4), ('별로', 3), ('다', 3), ('이', 3), ('너무', 3), ('음식', 3), ('서비스', 3), ('하다', 2), ('방문', 2), ('위생', 2), ('좀', 2), ('더', 2), ('에', 2), ('조금', 2), ('정말', 1), ('맛있다', 1), ('추천', 1), ('기대하다', 1), ('것', 1), ('보단', 1), ('가격', 1), ('비싸다', 1), ('다시', 1), ('가다', 1), ('싶다', 1), ('생각', 1), ('안', 1), ('드네', 1), ('요', 1), ('완전', 1), ('최고', 1), ('!', 1), ('재', 1), ('의사', 1), ('있다', 1), ('만족스럽다', 1), ('상태', 1), ('가', 1), ('개선', 1), ('되다', 1), ('기르다', 1), ('바라다', 1), ('맛', 1), ('직원', 1), ('분들', 1), ('친절하다', 1), ('기념일', 1), ('분위기', 1), ('전반', 1), ('적', 1), ('으로', 1), ('짜다', 1), ('저', 1), ('는', 1), ('신경', 1), ('써다', 1), ('불쾌하다', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaK_i3zL2vO3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0b3a81e-2c36-4c06-cdd7-f9bdae795846"
      },
      "source": [
        "# 책갈피 🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥\n",
        "w2i = {}\n",
        "for pair in tqdm(word_count):\n",
        "  if pair[0] not in w2i:\n",
        "    w2i[pair[0]] = len(w2i)\n",
        "\n",
        "# w2i의 키값 관계를 뒤바꾼 버전\n",
        "# w2i.items() -> \n",
        "# [('.', 0), ('도', 1)... : 키값이 튜플형식으로 담긴 배열 리턴\n",
        "i2w={v:k for k,v in w2i.items()}"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 60/60 [00:00<00:00, 411879.28it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiGqiEGDL5B_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55e43200-1326-4169-ad01-043e732ba26f"
      },
      "source": [
        "print(train_tokenized)\n",
        "print(w2i)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['정말', '맛있다', '.', '추천', '하다', '.'], ['기대하다', '것', '보단', '별로', '이다', '.'], ['다', '좋다', '가격', '이', '너무', '비싸다', '다시', '가다', '싶다', '생각', '이', '안', '드네', '요', '.'], ['완전', '최고', '이다', '!', '재', '방문', '의사', '있다', '.'], ['음식', '도', '서비스', '도', '다', '만족스럽다', '.'], ['위생', '상태', '가', '좀', '별로', '이다', '.', '좀', '더', '개선', '되다', '기르다', '바라다', '.'], ['맛', '도', '좋다', '직원', '분들', '서비스', '도', '너무', '친절하다', '.'], ['기념일', '에', '방문', '하다', '음식', '도', '분위기', '도', '서비스', '도', '다', '좋다', '.'], ['전반', '적', '으로', '음식', '이', '너무', '짜다', '.', '저', '는', '별로', '이다', '.'], ['위생', '에', '조금', '더', '신경', '써다', '좋다', '.', '조금', '불쾌하다', '.']]\n",
            "{'.': 0, '도': 1, '이다': 2, '좋다': 3, '별로': 4, '다': 5, '이': 6, '너무': 7, '음식': 8, '서비스': 9, '하다': 10, '방문': 11, '위생': 12, '좀': 13, '더': 14, '에': 15, '조금': 16, '정말': 17, '맛있다': 18, '추천': 19, '기대하다': 20, '것': 21, '보단': 22, '가격': 23, '비싸다': 24, '다시': 25, '가다': 26, '싶다': 27, '생각': 28, '안': 29, '드네': 30, '요': 31, '완전': 32, '최고': 33, '!': 34, '재': 35, '의사': 36, '있다': 37, '만족스럽다': 38, '상태': 39, '가': 40, '개선': 41, '되다': 42, '기르다': 43, '바라다': 44, '맛': 45, '직원': 46, '분들': 47, '친절하다': 48, '기념일': 49, '분위기': 50, '전반': 51, '적': 52, '으로': 53, '짜다': 54, '저': 55, '는': 56, '신경': 57, '써다': 58, '불쾌하다': 59}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ttr52nB__q7r"
      },
      "source": [],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rcm_L4iJBufO"
      },
      "source": [
        "### 다음은 Word2Vec을 학습시키는 대표적인 방법인 Skipgram과 CBow를 다룹니다. \n",
        "\n",
        "* Cbow는 주변단어를 이용해, 주어진 단어를 예측하는 방법입니다.\n",
        "* Skipgram은 중심 단어를 이용하여 주변 단어를 예측하는 방법입니다.\n",
        "* 즉 데이터셋을 구성할때, input x 와 target y를 어떻게 설정하는지에 차이가 있습니다.\n",
        "\n",
        "참고자료 \n",
        "\n",
        "* https://simonezz.tistory.com/35 \n",
        "\n",
        "* https://towardsdatascience.com/nlp-101-word2vec-skip-gram-and-cbow-93512ee24314 \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXA5zaPPM3Wd"
      },
      "source": [
        "실제 모델에 들어가기 위한 input을 만들기 위해 `Dataset` 클래스를 정의합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s47ssyVt89t1"
      },
      "source": [
        "# CBOW : Continuous Bag Of Word\n",
        "class CBOWDataset(Dataset):\n",
        "  def __init__(self, train_tokenized, window_size=2):\n",
        "    self.x = [] # input word\n",
        "    self.y = [] # target word\n",
        "\n",
        "    for tokens in tqdm(train_tokenized): # train_tokenized,tokens : [] <- 각 문장이 형태소 단위로 분절된 채 배열에 담긴 상태\n",
        "      token_ids = [w2i[token] for token in tokens] # token : tokens 각 문장 내 형태소들\n",
        "      for i, id in enumerate(token_ids):\n",
        "        if i-window_size >= 0 and i+window_size < len(token_ids):\n",
        "          # 변별지역\n",
        "          self.x.append(token_ids[i-window_size:i] + token_ids[i+1:i+window_size+1])\n",
        "          self.y.append(id)\n",
        "          #\n",
        "\n",
        "    self.x = torch.LongTensor(self.x)  # (전체 데이터 개수, 2 * window_size)\n",
        "    self.y = torch.LongTensor(self.y)  # (전체 데이터 개수)\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.x.shape[0]\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.x[idx], self.y[idx]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvInhQ33AMJv"
      },
      "source": [
        "class SkipGramDataset(Dataset):\n",
        "  def __init__(self, train_tokenized, window_size=2):\n",
        "    self.x = []\n",
        "    self.y = []\n",
        "\n",
        "    for tokens in tqdm(train_tokenized):\n",
        "      token_ids = [w2i[token] for token in tokens]\n",
        "      for i, id in enumerate(token_ids):\n",
        "        if i-window_size >= 0 and i+window_size < len(token_ids):\n",
        "          # 변별지역\n",
        "          self.y += (token_ids[i-window_size:i] + token_ids[i+1:i+window_size+1])\n",
        "          self.x += [id] * 2 * window_size\n",
        "          #\n",
        "\n",
        "    self.x = torch.LongTensor(self.x)  # (전체 데이터 개수)\n",
        "    self.y = torch.LongTensor(self.y)  # (전체 데이터 개수)\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.x.shape[0]\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.x[idx], self.y[idx]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyAGV5IUUba0"
      },
      "source": [
        "각 모델에 맞는 `Dataset` 객체를 생성합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ep7Hm6oBWyy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9704f41d-9aa8-48ac-efaa-1ba567f4209d"
      },
      "source": [
        "cbow_set = CBOWDataset(train_tokenized)\n",
        "skipgram_set = SkipGramDataset(train_tokenized)\n",
        "print(list(skipgram_set))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 49461.13it/s]\n",
            "100%|██████████| 10/10 [00:00<00:00, 47608.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(tensor(0), tensor(17)), (tensor(0), tensor(18)), (tensor(0), tensor(19)), (tensor(0), tensor(10)), (tensor(19), tensor(18)), (tensor(19), tensor(0)), (tensor(19), tensor(10)), (tensor(19), tensor(0)), (tensor(22), tensor(20)), (tensor(22), tensor(21)), (tensor(22), tensor(4)), (tensor(22), tensor(2)), (tensor(4), tensor(21)), (tensor(4), tensor(22)), (tensor(4), tensor(2)), (tensor(4), tensor(0)), (tensor(23), tensor(5)), (tensor(23), tensor(3)), (tensor(23), tensor(6)), (tensor(23), tensor(7)), (tensor(6), tensor(3)), (tensor(6), tensor(23)), (tensor(6), tensor(7)), (tensor(6), tensor(24)), (tensor(7), tensor(23)), (tensor(7), tensor(6)), (tensor(7), tensor(24)), (tensor(7), tensor(25)), (tensor(24), tensor(6)), (tensor(24), tensor(7)), (tensor(24), tensor(25)), (tensor(24), tensor(26)), (tensor(25), tensor(7)), (tensor(25), tensor(24)), (tensor(25), tensor(26)), (tensor(25), tensor(27)), (tensor(26), tensor(24)), (tensor(26), tensor(25)), (tensor(26), tensor(27)), (tensor(26), tensor(28)), (tensor(27), tensor(25)), (tensor(27), tensor(26)), (tensor(27), tensor(28)), (tensor(27), tensor(6)), (tensor(28), tensor(26)), (tensor(28), tensor(27)), (tensor(28), tensor(6)), (tensor(28), tensor(29)), (tensor(6), tensor(27)), (tensor(6), tensor(28)), (tensor(6), tensor(29)), (tensor(6), tensor(30)), (tensor(29), tensor(28)), (tensor(29), tensor(6)), (tensor(29), tensor(30)), (tensor(29), tensor(31)), (tensor(30), tensor(6)), (tensor(30), tensor(29)), (tensor(30), tensor(31)), (tensor(30), tensor(0)), (tensor(2), tensor(32)), (tensor(2), tensor(33)), (tensor(2), tensor(34)), (tensor(2), tensor(35)), (tensor(34), tensor(33)), (tensor(34), tensor(2)), (tensor(34), tensor(35)), (tensor(34), tensor(11)), (tensor(35), tensor(2)), (tensor(35), tensor(34)), (tensor(35), tensor(11)), (tensor(35), tensor(36)), (tensor(11), tensor(34)), (tensor(11), tensor(35)), (tensor(11), tensor(36)), (tensor(11), tensor(37)), (tensor(36), tensor(35)), (tensor(36), tensor(11)), (tensor(36), tensor(37)), (tensor(36), tensor(0)), (tensor(9), tensor(8)), (tensor(9), tensor(1)), (tensor(9), tensor(1)), (tensor(9), tensor(5)), (tensor(1), tensor(1)), (tensor(1), tensor(9)), (tensor(1), tensor(5)), (tensor(1), tensor(38)), (tensor(5), tensor(9)), (tensor(5), tensor(1)), (tensor(5), tensor(38)), (tensor(5), tensor(0)), (tensor(40), tensor(12)), (tensor(40), tensor(39)), (tensor(40), tensor(13)), (tensor(40), tensor(4)), (tensor(13), tensor(39)), (tensor(13), tensor(40)), (tensor(13), tensor(4)), (tensor(13), tensor(2)), (tensor(4), tensor(40)), (tensor(4), tensor(13)), (tensor(4), tensor(2)), (tensor(4), tensor(0)), (tensor(2), tensor(13)), (tensor(2), tensor(4)), (tensor(2), tensor(0)), (tensor(2), tensor(13)), (tensor(0), tensor(4)), (tensor(0), tensor(2)), (tensor(0), tensor(13)), (tensor(0), tensor(14)), (tensor(13), tensor(2)), (tensor(13), tensor(0)), (tensor(13), tensor(14)), (tensor(13), tensor(41)), (tensor(14), tensor(0)), (tensor(14), tensor(13)), (tensor(14), tensor(41)), (tensor(14), tensor(42)), (tensor(41), tensor(13)), (tensor(41), tensor(14)), (tensor(41), tensor(42)), (tensor(41), tensor(43)), (tensor(42), tensor(14)), (tensor(42), tensor(41)), (tensor(42), tensor(43)), (tensor(42), tensor(44)), (tensor(43), tensor(41)), (tensor(43), tensor(42)), (tensor(43), tensor(44)), (tensor(43), tensor(0)), (tensor(3), tensor(45)), (tensor(3), tensor(1)), (tensor(3), tensor(46)), (tensor(3), tensor(47)), (tensor(46), tensor(1)), (tensor(46), tensor(3)), (tensor(46), tensor(47)), (tensor(46), tensor(9)), (tensor(47), tensor(3)), (tensor(47), tensor(46)), (tensor(47), tensor(9)), (tensor(47), tensor(1)), (tensor(9), tensor(46)), (tensor(9), tensor(47)), (tensor(9), tensor(1)), (tensor(9), tensor(7)), (tensor(1), tensor(47)), (tensor(1), tensor(9)), (tensor(1), tensor(7)), (tensor(1), tensor(48)), (tensor(7), tensor(9)), (tensor(7), tensor(1)), (tensor(7), tensor(48)), (tensor(7), tensor(0)), (tensor(11), tensor(49)), (tensor(11), tensor(15)), (tensor(11), tensor(10)), (tensor(11), tensor(8)), (tensor(10), tensor(15)), (tensor(10), tensor(11)), (tensor(10), tensor(8)), (tensor(10), tensor(1)), (tensor(8), tensor(11)), (tensor(8), tensor(10)), (tensor(8), tensor(1)), (tensor(8), tensor(50)), (tensor(1), tensor(10)), (tensor(1), tensor(8)), (tensor(1), tensor(50)), (tensor(1), tensor(1)), (tensor(50), tensor(8)), (tensor(50), tensor(1)), (tensor(50), tensor(1)), (tensor(50), tensor(9)), (tensor(1), tensor(1)), (tensor(1), tensor(50)), (tensor(1), tensor(9)), (tensor(1), tensor(1)), (tensor(9), tensor(50)), (tensor(9), tensor(1)), (tensor(9), tensor(1)), (tensor(9), tensor(5)), (tensor(1), tensor(1)), (tensor(1), tensor(9)), (tensor(1), tensor(5)), (tensor(1), tensor(3)), (tensor(5), tensor(9)), (tensor(5), tensor(1)), (tensor(5), tensor(3)), (tensor(5), tensor(0)), (tensor(53), tensor(51)), (tensor(53), tensor(52)), (tensor(53), tensor(8)), (tensor(53), tensor(6)), (tensor(8), tensor(52)), (tensor(8), tensor(53)), (tensor(8), tensor(6)), (tensor(8), tensor(7)), (tensor(6), tensor(53)), (tensor(6), tensor(8)), (tensor(6), tensor(7)), (tensor(6), tensor(54)), (tensor(7), tensor(8)), (tensor(7), tensor(6)), (tensor(7), tensor(54)), (tensor(7), tensor(0)), (tensor(54), tensor(6)), (tensor(54), tensor(7)), (tensor(54), tensor(0)), (tensor(54), tensor(55)), (tensor(0), tensor(7)), (tensor(0), tensor(54)), (tensor(0), tensor(55)), (tensor(0), tensor(56)), (tensor(55), tensor(54)), (tensor(55), tensor(0)), (tensor(55), tensor(56)), (tensor(55), tensor(4)), (tensor(56), tensor(0)), (tensor(56), tensor(55)), (tensor(56), tensor(4)), (tensor(56), tensor(2)), (tensor(4), tensor(55)), (tensor(4), tensor(56)), (tensor(4), tensor(2)), (tensor(4), tensor(0)), (tensor(16), tensor(12)), (tensor(16), tensor(15)), (tensor(16), tensor(14)), (tensor(16), tensor(57)), (tensor(14), tensor(15)), (tensor(14), tensor(16)), (tensor(14), tensor(57)), (tensor(14), tensor(58)), (tensor(57), tensor(16)), (tensor(57), tensor(14)), (tensor(57), tensor(58)), (tensor(57), tensor(3)), (tensor(58), tensor(14)), (tensor(58), tensor(57)), (tensor(58), tensor(3)), (tensor(58), tensor(0)), (tensor(3), tensor(57)), (tensor(3), tensor(58)), (tensor(3), tensor(0)), (tensor(3), tensor(16)), (tensor(0), tensor(58)), (tensor(0), tensor(3)), (tensor(0), tensor(16)), (tensor(0), tensor(59)), (tensor(16), tensor(3)), (tensor(16), tensor(0)), (tensor(16), tensor(59)), (tensor(16), tensor(0))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QSo73PoRyd9"
      },
      "source": [
        "### **모델 Class 구현**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnnk44R6R28x"
      },
      "source": [
        "차례대로 두 가지 Word2Vec 모델을 구현합니다.  \n",
        "\n",
        "\n",
        "*   `self.embedding`: `vocab_size` 크기의 one-hot vector를 특정 크기의 `dim` 차원으로 embedding 시키는 layer.\n",
        "*   `self.linear`: 변환된 embedding vector를 다시 원래 `vocab_size`로 바꾸는 layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_HP1ISq5CWv"
      },
      "source": [
        "class CBOW(nn.Module):\n",
        "  def __init__(self, vocab_size, dim):\n",
        "    super(CBOW, self).__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, dim, sparse=True)\n",
        "    self.linear = nn.Linear(dim, vocab_size)\n",
        "\n",
        "  # B: batch size, W: window size, d_w: word embedding size, V: vocab size\n",
        "  def forward(self, x):  # x: (B, 2W)\n",
        "    embeddings = self.embedding(x)  # (B, 2W, d_w)\n",
        "    embeddings = torch.sum(embeddings, dim=1)  # (B, d_w)\n",
        "    output = self.linear(embeddings)  # (B, V)\n",
        "    return output"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQAUApww68MJ"
      },
      "source": [
        "class SkipGram(nn.Module):\n",
        "  def __init__(self, vocab_size, dim):\n",
        "    super(SkipGram, self).__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, dim, sparse=True)\n",
        "    self.linear = nn.Linear(dim, vocab_size)\n",
        "\n",
        "  # B: batch size, W: window size, d_w: word embedding size, V: vocab size\n",
        "  def forward(self, x): # x: (B)\n",
        "    embeddings = self.embedding(x)  # (B, d_w)\n",
        "    output = self.linear(embeddings)  # (B, V)\n",
        "    return output"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58cJalkDWYMT"
      },
      "source": [
        "두 가지 모델을 생성합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vWUXEi8WeM-"
      },
      "source": [
        "cbow = CBOW(vocab_size=len(w2i), dim=256)\n",
        "skipgram = SkipGram(vocab_size=len(w2i), dim=256)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxP7qdtNWil1"
      },
      "source": [
        "### **모델 학습**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVggZrQ4WpBS"
      },
      "source": [
        "다음과 같이 hyperparamter를 세팅하고 `DataLoader` 객체를 만듭니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygVdz5rSBeNu"
      },
      "source": [
        "batch_size=4\n",
        "learning_rate = 5e-4\n",
        "num_epochs = 5\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "cbow_loader = DataLoader(cbow_set, batch_size=batch_size)\n",
        "skipgram_loader = DataLoader(skipgram_set, batch_size=batch_size)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekixqKB3X5C1"
      },
      "source": [
        "첫번째로 CBOW 모델 학습입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-d95qR7oC822",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5696e67-8fff-47e5-f942-2948ae5e668e"
      },
      "source": [
        "cbow.train()\n",
        "cbow = cbow.to(device)\n",
        "optim = torch.optim.SGD(cbow.parameters(), lr=learning_rate)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "for e in range(1, num_epochs+1):\n",
        "  print(\"#\" * 50)\n",
        "  print(f\"Epoch: {e}\")\n",
        "  for batch in tqdm(cbow_loader):\n",
        "    x, y = batch\n",
        "    x, y = x.to(device), y.to(device) # (B, W), (B)\n",
        "    output = cbow(x)  # (B, V)\n",
        " \n",
        "    optim.zero_grad()\n",
        "    loss = loss_function(output, y)\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "\n",
        "    print(f\"Train loss: {loss.item()}\")\n",
        "\n",
        "print(\"Finished.\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##################################################\n",
            "Epoch: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:04<00:00,  3.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 4.367950439453125\n",
            "Train loss: 4.081388473510742\n",
            "Train loss: 5.919615268707275\n",
            "Train loss: 4.6237945556640625\n",
            "Train loss: 4.997145652770996\n",
            "Train loss: 4.637975692749023\n",
            "Train loss: 5.0959320068359375\n",
            "Train loss: 4.78775691986084\n",
            "Train loss: 4.848222732543945\n",
            "Train loss: 4.347129821777344\n",
            "Train loss: 4.189299583435059\n",
            "Train loss: 4.047170639038086\n",
            "Train loss: 4.393255710601807\n",
            "Train loss: 4.840418815612793\n",
            "Train loss: 4.492781639099121\n",
            "Train loss: 5.045507431030273\n",
            "##################################################\n",
            "Epoch: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:00<00:00, 229.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 4.172040939331055\n",
            "Train loss: 3.9550180435180664\n",
            "Train loss: 5.791236877441406\n",
            "Train loss: 4.500115394592285\n",
            "Train loss: 4.860576629638672\n",
            "Train loss: 4.366282939910889\n",
            "Train loss: 4.913092136383057\n",
            "Train loss: 4.645573616027832\n",
            "Train loss: 4.739428520202637\n",
            "Train loss: 4.193831443786621\n",
            "Train loss: 4.021744728088379\n",
            "Train loss: 3.7034215927124023\n",
            "Train loss: 4.257310390472412\n",
            "Train loss: 4.724953651428223\n",
            "Train loss: 4.334661483764648\n",
            "Train loss: 4.903207778930664\n",
            "##################################################\n",
            "Epoch: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:00<00:00, 402.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 3.9816129207611084\n",
            "Train loss: 3.8307442665100098\n",
            "Train loss: 5.6647186279296875\n",
            "Train loss: 4.377792835235596\n",
            "Train loss: 4.725620269775391\n",
            "Train loss: 4.101956844329834\n",
            "Train loss: 4.734466552734375\n",
            "Train loss: 4.505661964416504\n",
            "Train loss: 4.6349711418151855\n",
            "Train loss: 4.0444440841674805\n",
            "Train loss: 3.8604066371917725\n",
            "Train loss: 3.3774802684783936\n",
            "Train loss: 4.123623371124268\n",
            "Train loss: 4.611520767211914\n",
            "Train loss: 4.179309844970703\n",
            "Train loss: 4.763580322265625\n",
            "##################################################\n",
            "Epoch: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/16 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 3.796795606613159\n",
            "Train loss: 3.7086355686187744\n",
            "Train loss: 5.540072441101074\n",
            "Train loss: 4.256858825683594\n",
            "Train loss: 4.5922956466674805\n",
            "Train loss: 3.846318483352661\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r100%|██████████| 16/16 [00:00<00:00, 455.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 4.560283660888672\n",
            "Train loss: 4.368045806884766\n",
            "Train loss: 4.534875392913818\n",
            "Train loss: 3.8992550373077393\n",
            "Train loss: 3.706021547317505\n",
            "Train loss: 3.072640895843506\n",
            "Train loss: 3.9922869205474854\n",
            "Train loss: 4.500113487243652\n",
            "Train loss: 4.026960372924805\n",
            "Train loss: 4.6266069412231445\n",
            "##################################################\n",
            "Epoch: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/16 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 3.6177618503570557\n",
            "Train loss: 3.588758945465088\n",
            "Train loss: 5.417302131652832\n",
            "Train loss: 4.137350082397461\n",
            "Train loss: 4.460622787475586\n",
            "Train loss: 3.600808620452881\n",
            "Train loss: 4.390758514404297\n",
            "Train loss: 4.232745170593262\n",
            "Train loss: 4.438979148864746\n",
            "Train loss: 3.7585155963897705\n",
            "Train loss: 3.559319496154785\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r100%|██████████| 16/16 [00:00<00:00, 269.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 2.7919657230377197\n",
            "Train loss: 3.8633906841278076\n",
            "Train loss: 4.390720367431641\n",
            "Train loss: 3.87786602973938\n",
            "Train loss: 4.492275714874268\n",
            "Finished.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDahBf6IX4py"
      },
      "source": [
        "다음으로 Skip-gram 모델 학습입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJxGEusqFV5r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc53659e-ab3a-4cf9-a739-d7a51bee6de7"
      },
      "source": [
        "skipgram.train()\n",
        "skipgram = skipgram.to(device)\n",
        "optim = torch.optim.SGD(skipgram.parameters(), lr=learning_rate)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "for e in range(1, num_epochs+1):\n",
        "  print(\"#\" * 50)\n",
        "  print(f\"Epoch: {e}\")\n",
        "  for batch in tqdm(skipgram_loader):\n",
        "    x, y = batch\n",
        "    x, y = x.to(device), y.to(device) # (B, W), (B)\n",
        "    output = skipgram(x)  # (B, V)\n",
        "\n",
        "    optim.zero_grad()\n",
        "    loss = loss_function(output, y)\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "\n",
        "  print(f\"Train loss: {loss.item()}\")\n",
        "\n",
        "print(\"Finished.\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##################################################\n",
            "Epoch: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 64/64 [00:00<00:00, 399.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 4.906717300415039\n",
            "##################################################\n",
            "Epoch: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 64/64 [00:00<00:00, 549.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 4.852550506591797\n",
            "##################################################\n",
            "Epoch: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 64/64 [00:00<00:00, 808.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 4.798703670501709\n",
            "##################################################\n",
            "Epoch: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 64/64 [00:00<00:00, 654.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 4.745182991027832\n",
            "##################################################\n",
            "Epoch: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 64/64 [00:00<00:00, 530.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 4.691995620727539\n",
            "Finished.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pi0sbHV6dEOR"
      },
      "source": [
        "### **테스트**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGarLWxXeJvz"
      },
      "source": [
        "학습된 각 모델을 이용하여 test 단어들의 word embedding을 확인합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4A1wrl-L_RjF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bfae454-f268-4744-cbff-f01d9596c653"
      },
      "source": [
        "for word in test_words:\n",
        "  input_id = torch.LongTensor([w2i[word]]).to(device)\n",
        "  emb = cbow.embedding(input_id)\n",
        "\n",
        "  print(f\"Word: {word}\")\n",
        "  print(emb.squeeze(0))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word: 음식\n",
            "tensor([ 0.6924,  1.3817,  0.1864,  0.3541,  0.7686, -1.6402, -0.1796,  0.6457,\n",
            "         0.1299,  1.2757,  0.8116, -0.3360, -0.8790,  0.3903, -0.0113, -0.2284,\n",
            "        -1.2019, -0.3848, -0.5456, -0.6028, -0.6682, -1.0583,  0.3694,  0.0648,\n",
            "        -1.0088, -1.1150, -0.4796, -0.6043, -0.4949,  1.2972, -1.5543, -0.9237,\n",
            "        -0.6792, -1.5852,  1.7448, -1.6431,  0.4418, -0.8992, -0.6649, -0.5384,\n",
            "         1.4459,  0.0404, -0.5555, -0.4267, -1.6670, -0.6329, -0.1922,  0.1141,\n",
            "         0.5955, -0.1649, -0.2175, -1.7560, -1.6086,  1.4294,  1.0764, -1.2164,\n",
            "        -0.4260,  1.2449, -0.0631,  1.2592, -0.4704, -0.3899,  0.0512,  0.0507,\n",
            "        -1.6572, -0.3617, -1.4285,  0.0220,  1.2191,  0.1334,  0.2535, -0.8667,\n",
            "        -0.3426,  0.3113, -1.5287,  0.3063, -0.5536, -1.1134, -1.2305, -0.0046,\n",
            "         1.9491, -0.5246,  0.8646,  0.9314,  0.5383, -0.0818, -0.1622,  0.4904,\n",
            "        -0.6091, -0.8279,  0.2874,  0.9205, -0.2289, -1.1459, -0.4419,  1.9516,\n",
            "        -0.2907,  0.0133,  0.7380, -0.1578,  0.4614,  0.8625,  1.2879,  0.3819,\n",
            "         0.7068,  0.1146, -1.3770, -1.0649,  1.0408, -0.0863, -1.2585,  1.1019,\n",
            "         0.0697,  0.0310,  1.5333,  0.5233,  0.7281, -0.7675, -1.3844,  0.8042,\n",
            "         2.2396,  0.0592,  0.4893, -0.8075, -0.0179, -1.1547, -0.2078, -0.7525,\n",
            "        -0.6941, -1.4883,  0.6041, -0.5299, -0.4862, -1.0837,  0.2272,  0.3880,\n",
            "         0.4574, -0.7436, -1.2163,  0.4051, -0.2532,  0.5888,  2.1467, -0.0682,\n",
            "        -0.3093,  0.0697,  0.2490, -0.5037, -0.4401,  0.2234,  0.2385,  0.2657,\n",
            "        -0.0831,  0.7144,  0.7539, -0.9294, -0.5638, -0.3923,  1.4058,  0.3728,\n",
            "         0.8349, -1.2350,  2.0118,  1.6945,  0.6953,  1.5458,  0.4931,  1.2178,\n",
            "        -1.0513, -0.1729, -1.0878,  1.0765, -0.1965,  2.1202, -1.3094, -0.5846,\n",
            "        -0.7065,  1.0514,  0.2881, -0.8466,  0.0116,  1.3014, -0.2192, -0.3417,\n",
            "        -0.4143,  1.0512,  0.6287,  1.6163,  1.0770,  0.5110, -0.7167, -0.8816,\n",
            "        -0.3199, -0.2606, -2.0021,  0.7549,  0.1354,  0.0893, -1.3719,  0.5125,\n",
            "         0.1420, -0.1998,  2.3556,  0.2232,  0.4640, -1.2251,  0.1242,  1.1340,\n",
            "        -0.4451,  0.4505,  1.5021,  1.4545,  0.1131, -0.9536, -0.6745,  1.8987,\n",
            "        -0.5292, -0.7080, -2.0331,  1.0171,  0.4422, -1.3149,  0.1073, -0.3892,\n",
            "         0.1306,  0.8230, -0.1392,  0.5595, -2.2619,  1.0283, -0.5064,  1.9201,\n",
            "         1.8124, -0.5280,  0.4727, -2.1263, -1.2309, -1.7649,  0.1648, -1.7673,\n",
            "         0.7344,  0.0434, -0.3233,  0.4807, -0.1122, -1.7311,  2.3109, -1.5282,\n",
            "        -0.9229,  0.8702, -1.3619, -0.1854,  0.8364,  1.4419, -0.0708,  1.5277],\n",
            "       device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
            "Word: 맛\n",
            "tensor([-2.1954e+00,  6.0004e-01,  9.1658e-02, -7.0365e-01,  6.6251e-01,\n",
            "        -6.2839e-01, -2.0481e-01, -6.3408e-01, -3.9311e-01,  3.1070e-01,\n",
            "         1.8821e+00, -1.0558e+00, -5.2490e-01, -2.3804e+00, -1.5947e+00,\n",
            "         3.3454e-01,  2.2928e-01,  5.9605e-02,  9.3006e-02,  1.6006e+00,\n",
            "        -9.8659e-01, -1.4771e+00,  8.2991e-01, -9.5171e-01,  8.4781e-02,\n",
            "         8.3187e-01, -1.9428e-01,  1.1351e+00, -9.0590e-01, -4.7342e-01,\n",
            "        -3.0926e+00,  2.4228e+00, -6.7077e-01,  7.9949e-01, -4.7407e-01,\n",
            "        -2.3144e-01,  2.5407e-01,  1.8594e-01,  3.6940e-01, -2.9417e-01,\n",
            "         6.3018e-01,  3.7085e-01, -3.7404e-01, -1.4285e+00, -8.5394e-01,\n",
            "        -9.2336e-01, -4.2096e-01,  3.1310e-01, -1.6701e+00,  3.9794e-01,\n",
            "         1.6791e+00, -3.6643e-01, -9.8704e-01,  5.0872e-01, -7.8881e-02,\n",
            "         1.0321e+00,  1.5206e-01, -9.4116e-01,  1.7567e-02, -9.8125e-01,\n",
            "         3.5192e-01, -1.5387e+00, -4.9291e-01, -5.1258e-01,  2.9500e-01,\n",
            "         1.1295e+00, -6.0639e-01, -8.6215e-01,  2.4550e-01,  4.2784e-01,\n",
            "         6.7970e-01,  4.9144e-01,  1.2016e+00, -2.2094e-01,  1.6386e-01,\n",
            "         2.0505e-01,  2.1355e+00, -7.0702e-01, -4.0105e-01,  2.3530e-01,\n",
            "        -1.0759e+00, -9.6197e-01, -1.7362e+00, -8.4962e-05, -1.0474e-01,\n",
            "        -7.2468e-01,  3.3434e-01,  4.1193e-01,  5.4493e-01, -1.3191e+00,\n",
            "        -4.2457e-01, -5.2793e-01,  9.2441e-01,  5.6744e-01,  3.6939e-01,\n",
            "         8.9456e-01,  1.0018e+00, -4.3098e-01, -1.0473e+00, -3.3901e-01,\n",
            "        -1.4972e+00, -1.3243e+00, -8.2649e-01, -7.5279e-02,  1.3339e+00,\n",
            "        -1.1347e+00,  3.0581e-01, -1.5294e+00, -3.7162e-02,  2.3890e-01,\n",
            "        -1.9306e+00,  8.7640e-01,  1.3845e+00,  7.7541e-01, -7.4030e-01,\n",
            "         1.0879e-01,  6.3680e-01, -2.0569e-01, -3.3116e-01,  8.5300e-01,\n",
            "        -2.6008e-01, -1.9755e+00, -6.9358e-01,  2.0131e+00,  1.5926e-02,\n",
            "         5.5271e-01, -5.8070e-01,  2.8225e-01,  1.4781e+00,  3.9501e-01,\n",
            "         1.0111e-02,  3.0227e-01,  1.8017e-01, -5.2579e-01,  3.1908e-01,\n",
            "         8.5917e-01, -5.2853e-01,  6.9473e-01,  9.8534e-01, -1.1456e+00,\n",
            "         1.2839e+00, -9.3435e-02, -6.1787e-01,  5.6679e-01,  7.2496e-01,\n",
            "         1.4611e+00,  4.4798e-01,  6.7132e-01,  2.3762e+00,  6.1577e-01,\n",
            "         1.6929e+00, -1.0195e+00,  2.5886e-01, -3.4440e-01,  1.5052e+00,\n",
            "         1.2003e+00, -5.6073e-01,  5.0419e-01,  5.8183e-01,  9.2580e-01,\n",
            "         1.2686e+00,  1.0462e+00, -1.1840e+00,  1.4596e-01, -3.2371e-01,\n",
            "         2.7369e-01,  4.9896e-01, -1.8838e-01, -1.7718e+00, -8.4016e-01,\n",
            "         6.7150e-01,  5.9226e-02,  1.8035e+00, -2.2590e-01,  8.9950e-02,\n",
            "        -8.2185e-01, -8.7706e-01, -6.9848e-01,  5.8300e-01, -9.3025e-02,\n",
            "         5.7939e-01,  5.8575e-01, -5.8181e-01, -6.2665e-01, -1.1394e-01,\n",
            "         1.5212e-02, -1.1298e-01, -4.0527e-02,  8.1369e-01,  1.1867e+00,\n",
            "        -2.3946e+00, -2.6888e+00, -4.0940e-02, -2.3848e+00, -2.7480e+00,\n",
            "         1.6233e-01,  1.0444e+00, -1.2386e+00, -4.1911e-01,  1.2007e+00,\n",
            "         1.4337e-01, -3.9709e-01, -1.5057e+00,  3.4818e-01, -1.5165e-01,\n",
            "         8.4410e-02, -2.3001e-01, -1.9854e-01, -4.9853e-01, -1.3189e-01,\n",
            "         4.3935e-01, -4.1555e-01, -1.0538e+00, -1.2483e+00, -6.3119e-01,\n",
            "        -1.3318e+00,  3.3580e-01,  1.4113e+00, -7.8580e-02,  9.5244e-01,\n",
            "         6.4583e-01,  1.4845e-01,  8.4262e-01, -1.1547e+00, -9.2140e-01,\n",
            "         8.2099e-01, -4.0603e-01, -7.8720e-01, -3.8687e-01,  1.4625e+00,\n",
            "        -7.7590e-01,  6.9097e-01, -1.7491e+00,  4.2484e-01, -1.4351e-01,\n",
            "        -2.2953e+00, -1.4832e+00, -7.5105e-01,  5.5416e-01,  4.1496e-01,\n",
            "         4.7816e-01, -1.0369e+00, -8.7996e-01, -7.8152e-01, -1.7911e+00,\n",
            "        -4.2290e-01, -5.3222e-01,  8.2393e-01, -5.7183e-01,  2.0837e+00,\n",
            "        -6.6874e-02, -4.6159e-01,  6.4317e-01,  1.3721e+00,  1.6678e+00,\n",
            "        -2.3574e-01], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
            "Word: 서비스\n",
            "tensor([-0.1060,  0.9068,  0.0998, -1.6503,  1.2323, -0.7893,  0.7386, -0.7706,\n",
            "        -0.4772,  0.2830, -1.2297,  0.4999, -1.3353, -0.4456, -0.7822,  0.7880,\n",
            "        -0.3738,  0.9019, -0.2273, -0.0468, -1.6300, -0.3892, -0.8461,  0.2019,\n",
            "         1.7065,  0.4842, -1.0149, -0.8417,  0.6033, -1.1981,  0.6314, -0.2117,\n",
            "         0.8378,  1.0074, -1.3111,  1.0251,  0.0465,  1.2752,  0.2660,  0.2624,\n",
            "        -0.6668, -0.2471, -0.8101, -0.0848,  1.5391, -1.0387,  0.3435,  2.4482,\n",
            "        -1.1009, -0.1712,  0.4428,  0.0503,  0.7717, -0.5393, -0.1108, -0.0831,\n",
            "        -1.3353,  1.6211, -0.8783, -0.0538,  1.1672, -0.3097, -0.6624,  1.8372,\n",
            "         0.1585,  1.0940,  0.8075, -1.3593, -0.2497,  1.2246,  0.5741, -0.7667,\n",
            "         1.7495,  2.6470, -1.3740, -1.0725, -0.1425,  1.4131,  0.9506, -0.6607,\n",
            "         0.3060, -0.9028,  1.9620, -0.3643,  2.0958,  1.6089, -0.2485, -0.0616,\n",
            "        -1.6021, -0.1741,  0.1198, -0.8814, -0.3853,  0.0379,  1.1136,  0.1284,\n",
            "        -1.4107, -0.7826,  1.8853, -0.4286,  1.4858,  0.1223,  0.7400, -1.5377,\n",
            "         2.4760,  0.1416, -0.5392, -0.8335,  0.7919, -0.1115, -1.7834, -0.9737,\n",
            "        -0.5200,  0.8982, -0.0402, -0.4551, -2.0923, -0.9793, -1.0526, -1.0231,\n",
            "         0.4583, -1.2794,  1.6989,  0.4533, -0.7521, -0.8053, -0.1952,  0.3200,\n",
            "         0.1196, -0.7473, -1.1167, -1.0303, -0.9864, -1.0489, -0.4081,  0.1740,\n",
            "        -0.6713, -1.3198, -1.1382,  0.6612,  2.2997,  0.1245,  0.7489,  0.6336,\n",
            "         0.3727, -2.4365,  0.9685,  0.0243, -0.9561, -1.5393,  0.5389,  0.3131,\n",
            "        -0.6107,  0.3237,  0.4928,  0.3869, -0.9585,  0.4813, -0.3620, -0.7712,\n",
            "        -0.2529,  0.0521, -0.0626, -1.4891,  0.8876, -1.6765,  0.7349,  0.3428,\n",
            "         0.8094,  1.0794,  0.2003, -2.2232,  0.0372, -0.9423, -1.0443, -0.1487,\n",
            "        -0.5590, -1.9155,  0.2852,  1.0812, -0.2466, -0.5437, -1.1099,  0.6067,\n",
            "         0.4784, -0.2445, -0.1645,  0.8460,  2.3243, -0.3109, -0.6033,  1.9427,\n",
            "        -0.3610,  0.6502, -0.2900,  0.5686, -1.0356,  0.2245,  0.7954,  1.0550,\n",
            "         0.4717, -0.1249,  0.5832, -0.2021,  0.9595,  2.3580, -0.2889,  1.1840,\n",
            "         0.0903,  2.5058,  0.1116,  1.5917, -0.8172,  0.1847,  0.2172, -0.4988,\n",
            "         0.6304,  0.7158, -0.1474, -0.0088,  0.0778, -0.5309,  0.9001,  1.5326,\n",
            "         0.6334,  1.0239, -0.4435, -0.7201, -0.6353, -0.2844,  1.4217,  0.4993,\n",
            "         1.0887, -0.0731, -0.5620, -1.1177,  1.7793, -0.3880,  1.5826, -0.0612,\n",
            "         0.1386, -0.2045,  0.3742, -0.4268, -0.3724, -0.3140, -1.4627, -0.1912,\n",
            "        -0.3478,  0.3451,  0.7004, -1.2484,  1.6804, -2.2438, -0.7060,  1.1427],\n",
            "       device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
            "Word: 위생\n",
            "tensor([-0.1255, -1.4139,  1.2456,  0.8476, -0.8809, -0.8402,  1.3328,  1.0542,\n",
            "        -0.6279, -0.8900, -2.1700, -1.1880, -0.4593, -0.5171,  0.1764,  0.6677,\n",
            "        -0.9283, -0.8752,  1.8021, -1.0957,  0.8713, -0.0262,  1.0977, -0.4982,\n",
            "         0.3593, -0.7301,  0.3068, -0.4431, -0.6141,  0.6334,  1.4894,  0.3690,\n",
            "         0.6210, -0.7096,  1.3567,  0.0773, -0.4823,  1.3625, -1.8421,  0.3678,\n",
            "        -1.2294,  0.0448, -0.4607, -0.8917, -0.9185,  0.9182, -2.0466, -0.5440,\n",
            "         0.5012, -0.4877, -0.4325,  0.8270,  1.3359,  0.7312, -0.7993,  1.1174,\n",
            "         0.4607,  0.7321,  0.9686,  0.7186, -1.0264, -0.8065, -1.7510,  0.0761,\n",
            "         0.0694,  0.9863,  0.8110, -0.3560,  0.5731,  1.0047,  1.6087,  0.0929,\n",
            "        -0.6673, -1.1636, -0.2750, -0.2949,  0.0207, -1.0439,  1.3778, -0.8154,\n",
            "         1.7393,  1.2316,  0.2113, -0.6459,  1.0103,  0.2491, -0.8552,  1.3518,\n",
            "        -0.8377,  1.0474,  0.4042, -1.8952, -0.5106, -0.5403,  0.4352, -0.3584,\n",
            "        -0.8146,  0.1655, -1.4200, -1.1421,  0.2577, -0.4720,  1.0175, -2.1369,\n",
            "         0.0693, -1.0159, -1.2952,  0.8064, -1.5543, -2.2340, -0.8090,  2.7356,\n",
            "        -0.6485, -0.9887, -0.4688,  0.4626, -0.4818,  0.3200, -2.0480,  0.3027,\n",
            "        -0.3586, -0.9499,  0.0309,  0.5594, -0.1604,  0.4812, -0.2173, -0.5675,\n",
            "        -0.8702, -1.9741, -0.2544,  1.3301,  0.4981, -0.0197,  0.7486, -0.9325,\n",
            "        -1.0148, -2.1646,  0.6654, -1.3595,  0.0796, -1.2799,  1.3707, -0.5355,\n",
            "        -0.0139,  0.7202,  1.7466, -0.2794, -1.2294,  1.5379,  0.4435,  0.2991,\n",
            "         0.2677, -1.6617,  0.2899,  0.0392,  0.9840, -0.6927, -0.8727,  0.7538,\n",
            "         1.6382,  0.2314,  0.8687, -0.8165, -1.3511,  0.9469,  1.4414,  0.2375,\n",
            "         1.1081, -0.1811,  1.2047, -1.0014, -0.2377,  1.1572, -0.7966,  0.4641,\n",
            "        -0.1296,  1.8040, -2.5765, -0.9768,  0.0657, -0.5688,  0.8426,  0.4721,\n",
            "         0.6297, -2.6426,  1.8358, -1.1498, -1.0433,  0.0063,  0.1926, -0.9157,\n",
            "        -2.0545, -0.9677, -1.1907, -0.0818, -0.6974,  0.8943,  1.0527, -0.0190,\n",
            "        -0.8820,  0.2962, -0.5457,  1.0657, -0.3202,  0.3831,  0.2233,  0.3946,\n",
            "         0.1029, -1.0415, -0.3587,  0.0301, -2.1206, -0.1721,  0.0189,  1.1241,\n",
            "         0.7765,  1.5235,  0.2710, -1.9860, -1.0991, -0.8085, -0.9889,  0.8514,\n",
            "        -0.4307, -1.7862,  2.0467, -0.8536,  0.5562, -1.5047, -0.4980,  0.9608,\n",
            "        -0.0236, -1.7515,  0.6010, -1.4866, -0.0476,  2.0675, -0.9614,  1.5066,\n",
            "        -1.9021,  1.5680, -1.0159,  0.9396,  0.2356,  1.0750,  0.0644,  0.5751,\n",
            "        -0.4937,  0.0442, -0.2946, -0.8039,  0.9952, -0.6014, -0.2621,  0.2950],\n",
            "       device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
            "Word: 가격\n",
            "tensor([-7.1523e-02,  1.7040e+00,  3.8227e-01, -1.7280e-01, -1.1030e+00,\n",
            "        -4.7425e-01, -1.9934e-01,  1.7337e+00, -3.5909e-01, -6.5098e-01,\n",
            "         1.3774e+00, -1.2940e+00, -1.2564e+00,  1.4755e+00,  6.6198e-01,\n",
            "        -1.1593e-01, -4.2770e-01,  2.9705e-01,  7.9452e-01,  1.5128e-01,\n",
            "         8.1068e-01,  1.9536e-01, -4.0850e-01, -1.0222e+00, -1.6282e-01,\n",
            "        -1.4171e+00, -2.6170e+00, -1.1173e+00, -4.6598e-01,  4.1209e-02,\n",
            "         6.0748e-01,  3.7683e-01, -3.1112e-02,  4.3874e-01, -8.3416e-01,\n",
            "         9.1857e-01, -8.9324e-01, -5.8298e-01, -1.5790e-01, -5.5717e-01,\n",
            "        -5.2257e-01, -1.6960e+00,  2.3012e-01, -5.6985e-01, -2.4047e-01,\n",
            "         9.1181e-01, -7.5746e-01, -8.0166e-01,  1.5527e+00,  8.4895e-01,\n",
            "         1.3954e+00, -1.1700e+00,  3.1207e-01, -3.6552e-01,  9.2006e-01,\n",
            "        -3.6462e-01,  6.1873e-01,  2.6600e+00, -3.8187e-01, -2.1910e+00,\n",
            "        -6.8840e-01, -2.7820e-02, -1.1326e-02, -2.0665e+00, -1.1154e+00,\n",
            "        -5.2105e-01,  1.0315e-01, -1.4412e-01,  1.0323e+00,  6.7544e-01,\n",
            "         6.0587e-01, -3.9812e-01,  5.4248e-01, -8.2135e-01,  1.0027e-01,\n",
            "         6.4991e-01,  3.5249e-01, -6.5867e-01,  5.5310e-01,  6.9966e-01,\n",
            "         3.3097e-01,  3.0585e-01,  9.1218e-01,  3.9323e-01,  1.5177e+00,\n",
            "        -1.4499e+00, -8.4330e-01,  1.5480e+00, -9.8764e-01,  2.3422e+00,\n",
            "        -5.0049e-01,  4.7247e-01,  6.9893e-03,  7.2376e-01,  1.5941e-02,\n",
            "         8.5687e-01, -1.2485e+00, -3.1688e-01,  5.2802e-01, -1.8914e-01,\n",
            "         7.1698e-01, -8.6828e-01,  6.7375e-01,  1.0200e+00,  9.4554e-01,\n",
            "         6.8880e-01,  8.3759e-01,  9.6329e-01,  6.6727e-01, -6.4240e-01,\n",
            "        -8.6781e-01,  4.3304e-01, -1.9045e-01,  7.6779e-01,  3.3936e-01,\n",
            "        -4.1506e-01, -9.9715e-01,  7.3731e-01,  1.7049e-01, -2.1910e-01,\n",
            "        -1.9595e-01,  5.6991e-01, -1.2220e+00, -1.4461e+00,  4.2252e-01,\n",
            "        -6.7555e-01, -1.2262e+00, -1.0439e+00, -1.3942e-01,  4.0211e-01,\n",
            "         3.2370e-02,  1.2461e+00,  7.0856e-01,  1.3285e+00,  1.2131e-01,\n",
            "         1.2228e+00, -1.0990e+00,  6.3142e-01,  6.8638e-01, -3.8692e-01,\n",
            "         2.0253e+00,  3.4281e-01,  7.1732e-01, -3.5972e-02,  1.2334e+00,\n",
            "         3.4253e-01,  2.8878e-01, -1.6067e+00, -2.3763e-01,  3.8751e-01,\n",
            "         1.4068e+00,  5.8408e-01,  2.6767e-01,  1.1344e+00, -1.0535e-01,\n",
            "         7.1350e-02,  3.3256e-01,  3.8162e-01, -1.5359e+00, -1.1892e-02,\n",
            "        -2.0863e-01, -7.0115e-02, -3.9942e-03, -3.9432e-01,  3.2257e-01,\n",
            "         8.1456e-01,  4.7679e-01,  8.3528e-01,  2.3453e-02,  8.8913e-01,\n",
            "        -8.2315e-01, -8.0593e-01, -1.7544e+00,  6.6117e-02, -1.4621e+00,\n",
            "        -1.6614e+00, -1.5242e+00,  5.0691e-02, -1.4013e+00,  9.4190e-01,\n",
            "        -1.0994e+00,  1.2966e+00,  2.5580e-01,  2.6301e-01, -4.0611e-01,\n",
            "        -8.7710e-01, -9.8812e-01,  5.1952e-01,  3.6315e-01,  6.8091e-01,\n",
            "        -2.2893e+00,  1.1196e-01,  8.6360e-01,  7.4053e-01, -2.9329e+00,\n",
            "         2.0938e+00,  1.0981e+00, -9.5384e-01,  2.6733e-01, -9.2909e-01,\n",
            "        -1.6850e+00, -5.8092e-01, -1.2605e+00,  4.9713e-01,  1.3922e+00,\n",
            "        -5.7532e-01, -2.6674e-01, -3.7451e-01,  6.6047e-01,  3.0334e-02,\n",
            "        -1.1970e+00, -1.3407e-02, -1.3101e+00,  1.6337e-01,  3.8337e-01,\n",
            "        -6.1170e-01,  3.8517e-01, -4.6165e-01,  1.1663e+00,  9.2194e-01,\n",
            "        -7.2882e-01,  1.3921e+00, -3.6521e-02, -1.3103e+00,  7.4875e-01,\n",
            "        -2.7411e-02,  4.3815e-01,  1.0874e+00,  2.4140e-01, -6.0664e-03,\n",
            "         9.7874e-01, -7.6847e-02,  1.7300e-01,  1.0357e+00,  1.4863e+00,\n",
            "        -4.9214e-01, -5.2680e-01, -2.8855e-02, -1.0710e+00, -5.0058e-01,\n",
            "        -5.7100e-02, -6.7600e-01, -7.2104e-01,  1.1023e+00,  6.7478e-01,\n",
            "         4.6296e-04,  2.6195e+00,  5.6096e-02,  1.5214e+00, -8.8909e-01,\n",
            "         1.4074e+00, -1.6701e+00,  7.7631e-01,  7.0429e-01,  2.9681e-01,\n",
            "        -1.1858e+00], device='cuda:0', grad_fn=<SqueezeBackward1>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_l5cPRZZe-R4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5dc67594-1865-48f3-ccda-2e29b9efbc21"
      },
      "source": [
        "for word in test_words:\n",
        "  input_id = torch.LongTensor([w2i[word]]).to(device)\n",
        "  emb = skipgram.embedding(input_id)\n",
        "\n",
        "  print(f\"Word: {word}\")\n",
        "  print(max(emb.squeeze(0)))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word: 음식\n",
            "tensor(2.2970, device='cuda:0', grad_fn=<UnbindBackward0>)\n",
            "Word: 맛\n",
            "tensor(2.5633, device='cuda:0', grad_fn=<UnbindBackward0>)\n",
            "Word: 서비스\n",
            "tensor(2.9672, device='cuda:0', grad_fn=<UnbindBackward0>)\n",
            "Word: 위생\n",
            "tensor(2.9230, device='cuda:0', grad_fn=<UnbindBackward0>)\n",
            "Word: 가격\n",
            "tensor(2.7593, device='cuda:0', grad_fn=<UnbindBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4mv-fDF29Ha",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10efe1c7-3a81-4958-c92f-5f5df1b5c6c9"
      },
      "source": [
        "test_words"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['음식', '맛', '서비스', '위생', '가격']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jYY7xYd4vIR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "803d1376-d476-4f00-8463-e60cfbd0f8e5"
      },
      "source": [
        "\n",
        "i2w[25]"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'다시'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OBlu8O63CXx"
      },
      "source": [
        "def most_similar(word,top_k=5):\n",
        "  input_id = torch.LongTensor([w2i[word]]).to(device)\n",
        "  input_emb = skipgram.embedding(input_id)\n",
        "  score=torch.matmul(input_emb,skipgram.embedding.weight.transpose(1,0)).view(-1)\n",
        "\n",
        "  _,top_k_ids=torch.topk(score,top_k)\n",
        "\n",
        "  return [i2w[word_id.item()] for word_id in top_k_ids][1:]"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c75HQoLn2_Ty",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8a00900-8461-4f07-9627-ab34c2bccbf3"
      },
      "source": [
        "most_similar(\"가격\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['위생', '신경', '서비스', '에']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YvYEGNr2IiR"
      },
      "source": [
        "## Word2Vec 시각화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcBVrabH2IiR"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8XdtYVf8Ydg"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "#matplotlib 패키지 한글 깨짐 처리 시작\n",
        "plt.rc('font', family='NanumBarunGothic') \n",
        "#plt.rc('font', family='AppleGothic') #맥"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEypFpsw7K8q"
      },
      "source": [
        "pca=PCA(n_components=2)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NAllD3y7NFo"
      },
      "source": [
        "pc_weight=pca.fit_transform(skipgram.embedding.weight.data.cpu().numpy())"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KKgYYTa7Uh3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "12f5f05e-767d-435a-a306-3d0694b8c719"
      },
      "source": [
        "plt.figure(figsize=(15,15))\n",
        "\n",
        "for word_id,(x_coordinate,y_coordinate) in enumerate(pc_weight):\n",
        "  plt.scatter(x_coordinate,y_coordinate,color=\"blue\")\n",
        "  plt.annotate(i2w[word_id], (x_coordinate, y_coordinate))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.font_manager:findfont: Font family ['NanumBarunGothic'] not found. Falling back to DejaVu Sans.\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 46020 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 46020 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51060 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45796 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51060 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45796 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51339 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51339 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 48324 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 47196 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 48324 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 47196 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45320 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 47924 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45320 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 47924 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51020 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49885 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51020 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49885 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49436 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 48708 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49828 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49436 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 48708 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49828 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 54616 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 54616 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 48169 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 47928 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 48169 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 47928 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 50948 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49373 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 50948 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49373 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51328 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51328 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45908 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45908 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 50640 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 50640 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51312 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44552 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51312 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44552 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51221 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 47568 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51221 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 47568 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 47579 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51080 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 47579 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51080 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 52628 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 52380 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 52628 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 52380 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44592 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45824 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44592 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45824 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44163 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44163 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 48372 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45800 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 48372 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45800 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44032 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44201 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44032 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44201 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49912 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49912 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49884 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49884 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49910 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49910 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44033 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44033 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 50504 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 50504 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 46300 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45348 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 46300 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45348 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 50836 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 50836 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 50756 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51204 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 50756 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51204 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 52572 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44256 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 52572 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44256 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51116 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51116 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51032 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49324 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51032 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49324 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 47564 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51313 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 47101 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 47564 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51313 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 47101 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49345 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 53468 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49345 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 53468 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44060 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49440 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44060 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49440 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 46104 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 46104 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 47476 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 47476 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 48148 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 46972 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 48148 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 46972 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51649 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 50896 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51649 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 50896 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 48516 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 46308 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 48516 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 46308 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 52828 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51208 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 52828 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51208 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45392 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51068 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45392 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51068 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 48152 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 48152 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51201 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51201 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51004 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51004 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51676 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51676 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51200 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51200 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45716 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45716 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49888 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44221 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49888 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44221 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 50024 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 50024 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 48520 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 53132 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 48520 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 53132 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x1080 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PX4adG7B7qdV"
      },
      "source": [],
      "execution_count": 31,
      "outputs": []
    }
  ]
}